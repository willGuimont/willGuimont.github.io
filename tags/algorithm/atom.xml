<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
    <title>wigum::website - Algorithm</title>
    <subtitle>William Guimont-Martin&#x27;s personal website</subtitle>
    <link rel="self" type="application/atom+xml" href="https://willguimont.com/tags/algorithm/atom.xml"/>
    <link rel="alternate" type="text/html" href="https://willguimont.com/"/>
    <generator uri="https://www.getzola.org/">Zola</generator>
    <updated>2020-01-23T00:00:00+00:00</updated>
    <id>https://willguimont.com/tags/algorithm/atom.xml</id>
    <entry xml:lang="en">
        <title>On Learning</title>
        <published>2020-01-23T00:00:00+00:00</published>
        <updated>2020-01-23T00:00:00+00:00</updated>
        
        <author>
          <name>
            William Guimont-Martin
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://willguimont.com/blog/on-learning/"/>
        <id>https://willguimont.com/blog/on-learning/</id>
        
        <content type="html" xml:base="https://willguimont.com/blog/on-learning/">&lt;p&gt;This is just some rambles on learning…&lt;&#x2F;p&gt;
&lt;p&gt;I’m currently taking a class on deep neural networks. In the first class, we discussed multiple ways to have artificial intelligence systems.&lt;&#x2F;p&gt;
&lt;p&gt;One way would be to code hand-designed programs to accomplish the task. &lt;code&gt;If&lt;&#x2F;code&gt; something happens, &lt;code&gt;then&lt;&#x2F;code&gt; the system does that, &lt;code&gt;else if&lt;&#x2F;code&gt; this other thing happens, &lt;code&gt;then&lt;&#x2F;code&gt; do this, &lt;code&gt;else if&lt;&#x2F;code&gt;… We can achieve great things using rule-based programs, but on more complex tasks we would need a lot of cases to handle a lot of different cases.&lt;&#x2F;p&gt;
&lt;p&gt;Another way would be to make the computer learn from the data. So we analyze the data, find features for the system to learn from, then use classical machine learning algorithms such as SVM. Again, for complex tasks, we may miss some useful feature to correctly learn from the data.&lt;&#x2F;p&gt;
&lt;p&gt;The next step is to make the algorithm find features by itself. This is what neural networks do. Each layer of a deep neural network extract features for the higher layers to “reason” about. The classical example is a neural network that detects faces. The first layers might detect very simple patterns like edges. Higher up, a layer might detect corners and contours. Layers at the end of the network might detect complex object parts used by the final layer to classify the object. This is what is called representation learning, the artificial intelligence system finds a way of representing the data so that it is easy to solve the task.&lt;sup class=&quot;footnote-reference&quot; id=&quot;fr-Goodfellow-1&quot;&gt;&lt;a href=&quot;#fn-Goodfellow&quot;&gt;1&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;A good way of visualizing the different takes on artificial intelligence is from the book Deep Learning&lt;sup class=&quot;footnote-reference&quot; id=&quot;fr-Goodfellow-2&quot;&gt;&lt;a href=&quot;#fn-Goodfellow&quot;&gt;1&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt;.
&lt;img src=&quot;https:&#x2F;&#x2F;willguimont.com&#x2F;blog&#x2F;on-learning&#x2F;types.webp&quot; alt=&quot;Types of learning&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;dreyfus&quot;&gt;Dreyfus&lt;&#x2F;h2&gt;
&lt;p&gt;When I learned about representation learning, it got me thinking about how we learn and the Dreyfus model&lt;sup class=&quot;footnote-reference&quot; id=&quot;fr-Dreyfus-1&quot;&gt;&lt;a href=&quot;#fn-Dreyfus&quot;&gt;2&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;I think we can draw links between the Novice stage and rule-based systems. In this first stage of the Dreyfus model, the learner relies on rules to determine the action.&lt;&#x2F;p&gt;
&lt;p&gt;Perhaps, there are other links between machine learning and the Dreyfus model.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;mental-representations&quot;&gt;Mental representations&lt;&#x2F;h2&gt;
&lt;p&gt;In The Sciences of the artificial&lt;sup class=&quot;footnote-reference&quot; id=&quot;fr-Simon-1&quot;&gt;&lt;a href=&quot;#fn-Simon&quot;&gt;3&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; from Herbert A. Simon, the author discusses the experience of A. de Groot and al. on chess perception. They showed chessboard with pieces in positions of real games for 5 seconds, then removed the pieces. Then they asked the subjects to put the pieces back in their position.&lt;&#x2F;p&gt;
&lt;p&gt;As you might guess, somebody not accustomed to chess would have a pretty hard time to reconstruct the positions. For grandmasters and masters though, they were able to reconstruct the positions with almost no error!&lt;&#x2F;p&gt;
&lt;p&gt;They then retried the experiment with &lt;em&gt;random&lt;&#x2F;em&gt; piece positions. In that case, even grandmasters could not reconstruct the board.&lt;&#x2F;p&gt;
&lt;p&gt;This shows that a grandmaster or a master built a good mental &lt;em&gt;representation&lt;&#x2F;em&gt; of the game. Instead of memorizing all pieces, they memorized relations between pieces, which allow them to memorize them more easily.&lt;&#x2F;p&gt;
&lt;p&gt;So, when I learned about representation learning, this reminded me of that particular example. Under the hood, a neural network &lt;em&gt;tries&lt;&#x2F;em&gt; to do exactly what the grandmaster did, build a good representation of the problem to solve. Once the representation is built, a simple linear layer can solve the problem!&lt;&#x2F;p&gt;
&lt;p&gt;I think this is what is really going on when humans learn. All the studying isn’t about memorizing facts or formulas, it’s about building a good mental representation that can be used to solve many problems. Once you understand basic algebra, you can use that framework to solve many problems you weren’t trained on.&lt;&#x2F;p&gt;
&lt;p&gt;In a neural network, we are usually not concerned about memorizing all the input data, instead, we want to generalize to unseen examples.&lt;&#x2F;p&gt;
&lt;p&gt;Perhaps we should tell students that they aren’t in school to memorize facts, but instead to build a toolbox of mental representations to help them in life. Our brain is a neural network that needs to be trained with great care!&lt;&#x2F;p&gt;
&lt;h2 id=&quot;sources&quot;&gt;Sources&lt;&#x2F;h2&gt;
&lt;section class=&quot;footnotes&quot;&gt;
&lt;ol class=&quot;footnotes-list&quot;&gt;
&lt;li id=&quot;fn-Goodfellow&quot;&gt;
&lt;p&gt;Goodfellow, Ian &amp;amp; Bengio, Yoshua &amp;amp; Courville, Aaron. Deep Learning. 2016. &lt;a href=&quot;#fr-Goodfellow-1&quot;&gt;↩&lt;&#x2F;a&gt; &lt;a href=&quot;#fr-Goodfellow-2&quot;&gt;↩2&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li id=&quot;fn-Dreyfus&quot;&gt;
&lt;p&gt;Dreyfus, Stuart E. The Five-Stage Model of Adult Skill Acquisition. 2004. &lt;a href=&quot;#fr-Dreyfus-1&quot;&gt;↩&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li id=&quot;fn-Simon&quot;&gt;
&lt;p&gt;Simon, Herbert A. The Sciences of the Artificial, 1969. &lt;a href=&quot;#fr-Simon-1&quot;&gt;↩&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;&#x2F;section&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>PROSAC</title>
        <published>2019-12-26T00:00:00+00:00</published>
        <updated>2019-12-26T00:00:00+00:00</updated>
        
        <author>
          <name>
            William Guimont-Martin
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://willguimont.com/blog/prosac-algorithm/"/>
        <id>https://willguimont.com/blog/prosac-algorithm/</id>
        
        <content type="html" xml:base="https://willguimont.com/blog/prosac-algorithm/">&lt;p&gt;Recently, for a graduated mobile robotics class, I had to present a scientific paper. Loving algorithms, I decided to read &lt;a href=&quot;2005-Matching-with-PROSAC-progressive-sample-consensus.pdf&quot; target=&quot;_blank&quot;&gt;Matching with PROSAC - Progressive Sample Concensus&lt;&#x2F;a&gt;&lt;sup class=&quot;footnote-reference&quot; id=&quot;fr-ChumMatas-1&quot;&gt;&lt;a href=&quot;#fn-ChumMatas&quot;&gt;1&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt;, a paper presenting a variant of the popular RANSAC algorithm.&lt;&#x2F;p&gt;
&lt;p&gt;This article will be a more detailed version of the presentation I gave for that class, the slides are &lt;a href=&quot;PROSAC.pdf&quot; target=&quot;_blank&quot;&gt;here&lt;&#x2F;a&gt;. I won’t go into much of the mathematics, the details are in the paper. What I want to do with this article is to give you the intuition behind the algorithm more than writing a lot of equations.&lt;&#x2F;p&gt;
&lt;p&gt;To supplement the presentation, I wrote an implementation of PROSAC in Python: &lt;a class=&quot;external&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;willGuimont&#x2F;PROSAC&quot; target=&quot;_blank&quot;&gt;willGuimont&#x2F;PROSAC&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Before jumping in PROSAC, let’s just review RANSAC a bit.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;ransac-random-sample-consensus&quot;&gt;RANSAC - Random sample consensus&lt;&#x2F;h2&gt;
&lt;p&gt;The main goal of RANSAC is to estimate a model from noised data with outliers.&lt;&#x2F;p&gt;
&lt;p&gt;The basic idea is to randomly sample points from all points; fit a model on those randomly chosen points; then check if the model fits with the rest of the data.&lt;&#x2F;p&gt;
&lt;p&gt;As you can see, the algorithm is relatively simple: sample, fit, check, rinse and repeat.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;pseudocode&quot;&gt;Pseudocode&lt;&#x2F;h3&gt;
&lt;p&gt;Let $m$ be the minimum required number of points to fit a model. For a linear model in 2D, $m$ would be equal to $2$.&lt;&#x2F;p&gt;
&lt;p&gt;Let $M$ a model to fit on the data.&lt;&#x2F;p&gt;
&lt;p&gt;Let $\epsilon_{tol}$ be the maximum error between a point and the model to be considered an inlier.&lt;&#x2F;p&gt;
&lt;p&gt;Let $\tau$ the fraction of inlier over the total number of points above which we are satisfied by the model. If we achieve that ratio, we stop early.&lt;&#x2F;p&gt;
&lt;p&gt;Let $w$ be the probability of a given point to be an inlier. This is used to estimate the number of times we have to run RANSAC before finding a satisfying solution.&lt;&#x2F;p&gt;
&lt;p&gt;Let $I$ be the set of inliers of maximal cardinality.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;ransac&quot;&gt;RANSAC&lt;&#x2F;h4&gt;
&lt;ol&gt;
&lt;li&gt;Sample $m$ points&lt;&#x2F;li&gt;
&lt;li&gt;Estimate a model $M$ from the $m$ sampled points&lt;&#x2F;li&gt;
&lt;li&gt;Find the number of inliers with model $M$ with tolerance $\epsilon_{tol}$. If the number of inliers is greater than the number of previously found inliers, replace $I$ with the new set of inliers.&lt;&#x2F;li&gt;
&lt;li&gt;If the fraction of inliers over the total number of points is greater than $\tau$, estimate the model with all inliers and stop.&lt;&#x2F;li&gt;
&lt;li&gt;Repeat steps 1 to 4, a maximum of $\frac{1}{w^m}$ times&lt;&#x2F;li&gt;
&lt;li&gt;After $\frac{1}{w^m}$ iterations, estimate the model with $I$.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h3 id=&quot;limitations-of-ransac&quot;&gt;Limitations of RANSAC&lt;&#x2F;h3&gt;
&lt;p&gt;In the paper, they looked at estimating epipolar geometry from a pair of pictures.&lt;&#x2F;p&gt;
&lt;div class=&quot;img-row&quot;&gt;
  &lt;img src=&quot;plant01.webp&quot; alt=&quot;Plant 1&quot; class=&quot;img-bg&quot; &#x2F;&gt;
  &lt;img src=&quot;plant02.webp&quot; alt=&quot;Plant 2&quot; class=&quot;img-bg&quot; &#x2F;&gt;
&lt;&#x2F;div&gt;
&lt;p&gt;With this pair of pictures, there is a lot of repetitive patterns on the floor and leaves, so there will be a lot of false matches. We have $m=7$ (epipolar geometry) and $w=9.2%$. The number of iteration of RANSAC is given by $\frac{1}{w^m}$. The estimated number of samples is over $8.43 \times 10^7$ !!! RANSAC would take a pretty long time before finding a good solution… That’s not realistic!&lt;&#x2F;p&gt;
&lt;p&gt;When the model is complex and the number of correct points is low, RANSAC often takes very long before finding a satisfying solution. Let’s see if PROSAC can help.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;prosac-progressive-sample-consensus&quot;&gt;PROSAC - Progressive sample consensus&lt;&#x2F;h2&gt;
&lt;p&gt;PROSAC adds the notion of quality to points with the basic assumption that points of greater quality have more chances to be correct.&lt;&#x2F;p&gt;
&lt;p&gt;So, to make the algorithm quicker, the algorithm starts by trying to fit with points of greater quality first. Then we &lt;em&gt;progressively&lt;&#x2F;em&gt; add points of lesser quality.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;intuition&quot;&gt;Intuition&lt;&#x2F;h3&gt;
&lt;p&gt;Let’s say that the points in red are outliers. We would like to fit a model of complexity $m=2$ on those points.&lt;&#x2F;p&gt;
&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;RANSAC&lt;&#x2F;th&gt;
      &lt;th&gt;PROSAC&lt;&#x2F;th&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;background-color:red;&quot;&gt;&lt;i&gt;$p_1$&lt;&#x2F;i&gt;&lt;&#x2F;td&gt;
      &lt;td&gt;&lt;i&gt;$p_2$&lt;&#x2F;i&gt;&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;i&gt;$p_2$&lt;&#x2F;i&gt;&lt;&#x2F;td&gt;
      &lt;td&gt;&lt;i&gt;$p_4$&lt;&#x2F;i&gt;&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;background-color:red;&quot;&gt;&lt;i&gt;$p_3$&lt;&#x2F;i&gt;&lt;&#x2F;td&gt;
      &lt;td style=&quot;background-color:red;&quot;&gt;&lt;i&gt;$p_6$&lt;&#x2F;i&gt;&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;i&gt;$p_4$&lt;&#x2F;i&gt;&lt;&#x2F;td&gt;
      &lt;td style=&quot;background-color:red;&quot;&gt;&lt;i&gt;$p_3$&lt;&#x2F;i&gt;&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;background-color:red;&quot;&gt;&lt;i&gt;$p_5$&lt;&#x2F;i&gt;&lt;&#x2F;td&gt;
      &lt;td&gt;&lt;i&gt;$p_5$&lt;&#x2F;i&gt;&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;i&gt;$p_6$&lt;&#x2F;i&gt;&lt;&#x2F;td&gt;
      &lt;td style=&quot;background-color:red;&quot;&gt;&lt;i&gt;$p_1$&lt;&#x2F;i&gt;&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;tbody&gt;
&lt;&#x2F;table&gt;
&lt;p&gt;In RANSAC, we would be sampling $2$ points among all points, so we would have a probability of $\frac{3}{6}\cdot\frac{2}{5}=20%$. So, we would need, on average, $5$ samples before finding an uncontaminated sample.&lt;&#x2F;p&gt;
&lt;p&gt;Whereas in PROSAC, we sort points by quality. The first points would be of higher quality. We would start sampling from the top 2 points, so we would sample $p_2$ and $p_4$, both of which are correct. In this case, we would have found an uncontaminated sample on the first draw.&lt;&#x2F;p&gt;
&lt;p&gt;Even with this simple example, we can see the possible speedup.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;simplified-pseudo-code&quot;&gt;Simplified pseudo code&lt;&#x2F;h3&gt;
&lt;h4 id=&quot;prosac&quot;&gt;PROSAC&lt;&#x2F;h4&gt;
&lt;ol&gt;
&lt;li&gt;Sort points by quality (highest quality first)&lt;&#x2F;li&gt;
&lt;li&gt;Consider the first $m$ points ($n\leftarrow m$)&lt;&#x2F;li&gt;
&lt;li&gt;Sample $m$ points from the top $n$&lt;&#x2F;li&gt;
&lt;li&gt;Fit the model&lt;&#x2F;li&gt;
&lt;li&gt;Verify model with all points&lt;&#x2F;li&gt;
&lt;li&gt;If the stopping criteria are not met, repeat steps 3 to 6, adding progressively points ($n\leftarrow n + 1$). Otherwise, fit the model on all inliers and terminate.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h3 id=&quot;quality&quot;&gt;Quality&lt;&#x2F;h3&gt;
&lt;p&gt;There are multiple ways of defining quality. For feature matching on images, we could use the correlation of intensity around features on each image.&lt;&#x2F;p&gt;
&lt;p&gt;In the paper, they mention Lowe’s distance. Lowe’s distance is the ratio between the most and second most similar matches.&lt;&#x2F;p&gt;
&lt;p&gt;$s_1$: distance of the most similar match&lt;&#x2F;p&gt;
&lt;p&gt;$s_2$: distance of the second most similar match&lt;&#x2F;p&gt;
&lt;p&gt;$\text{distance} = \frac{s_1}{s_2}$&lt;&#x2F;p&gt;
&lt;p&gt;This distance allows us to select the first points that are significantly better than any other. A match with a low Lowe’s distance has more chances of being correct.&lt;&#x2F;p&gt;
&lt;p&gt;To define quality, we could inverse the ratio.&lt;&#x2F;p&gt;
&lt;p&gt;$\text{quality} = \frac{s_2}{s_1}$&lt;&#x2F;p&gt;
&lt;p&gt;You can use any metric you want, as long as the higher the quality is, the higher the probability of the point being correct is.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;fast-sampling&quot;&gt;Fast sampling&lt;&#x2F;h3&gt;
&lt;p&gt;We could simply sample $m$ points from the top $n$ points, but that would be inefficient. We could sample multiple times the same set of points. So, to better explore the new possibles sets of $m$ points, we use the fact that when adding a new point $p_{n+1}$, we add a number $a$ of new samples. Each new sample contains the new point $p_{n+1}$ and $m-1$ from the $n$ first points.&lt;&#x2F;p&gt;
&lt;p&gt;So, we can simply sample $a$ times, picking $p_{n+1}$ and $m-1$ points in the best $n$ points. That way, we ensure the quickly explore the possibles sets of $m$ points.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;end-criterion&quot;&gt;End criterion&lt;&#x2F;h3&gt;
&lt;p&gt;The algorithm has two stopping criteria.&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Non-random solution&lt;&#x2F;li&gt;
&lt;li&gt;Maximality&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h3 id=&quot;non-random-solution&quot;&gt;Non-random solution&lt;&#x2F;h3&gt;
&lt;p&gt;This criterion states that for a solution to be considered correct, it must have more inliers than what would randomly happen for an incorrect model.&lt;&#x2F;p&gt;
&lt;p&gt;This is computed by estimating the probability that an incorrect model (fitted on a sample) is supported by a given point, not in the sample.&lt;&#x2F;p&gt;
&lt;p&gt;The solution is considered non-random if the probability of having its number of inliers by chance is smaller than a certain threshold (usually 5%).&lt;&#x2F;p&gt;
&lt;h3 id=&quot;maximality&quot;&gt;Maximality&lt;&#x2F;h3&gt;
&lt;p&gt;We want to stop sampling when the chance of getting a model that fit more points is lower than a certain threshold. This is computed by looking at the odds of missing a set of inliers bigger than previously found after a number of draws. If this probability falls under a certain threshold (usually 5%), it is not worth continuing drawing and we terminate.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;code&quot;&gt;Code&lt;&#x2F;h2&gt;
&lt;p&gt;Not having found any implementation of PROSAC online, I decided to post mine: &lt;a class=&quot;external&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;willGuimont&#x2F;PROSAC&quot; target=&quot;_blank&quot;&gt;willGuimont&#x2F;PROSAC&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;sources&quot;&gt;Sources&lt;&#x2F;h2&gt;
&lt;section class=&quot;footnotes&quot;&gt;
&lt;ol class=&quot;footnotes-list&quot;&gt;
&lt;li id=&quot;fn-ChumMatas&quot;&gt;
&lt;p&gt;Chum, Ondrej &amp;amp; Matas, Jiri. Matching with PROSAC - Progressive Sample Consensus. 2005. &lt;a href=&quot;#fr-ChumMatas-1&quot;&gt;↩&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;&#x2F;section&gt;
</content>
        
    </entry>
</feed>
