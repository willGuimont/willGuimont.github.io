[{"url":"https://willguimont.com/blog/","title":"wigum::blog","body":"wigum::blog\nWelcome to wigum::blog, where I write about just about anything that sparks my interest, from technology and programming to personal development and creativity."},{"url":"https://willguimont.com/blog/cancontrol/","title":"CanControl","body":"FRC (FIRST Robotics Competition) robots often use motor controllers like the REV Robotics Spark MAX, the CTRE Talon SRX, or the CTRE Victor SPX.\nHowever, these controllers are typically designed to work with specific libraries and hardware, such as the RoboRIO used in FRC.\nThis can make it challenging to integrate them with other microcontrollers like Arduino.\nCanControl is an Arduino library that enables communication with FRC-compatible motor controllers over the CAN bus.\nWith CanControl, you can send commands to motor controllers, read sensor data, and configure settings directly from an Arduino board.\nThis opens up new possibilities for robotics projects that want to leverage the capabilities of FRC motor controllers without being tied to the FRC ecosystem.\nGitHub: willGuimont/CanControl"},{"url":"https://willguimont.com/blog/silvascenes/","title":"SilvaScenes","body":"Co-authored paper: SilvaScenes: Tree Segmentation and Species Classification from Under-Canopy Images in Natural Forests\nInterest in robotics for forest management is growing, but perception in complex, natural environments remains a significant hurdle. Conditions such as heavy occlusion, variable lighting, and dense vegetation pose challenges to automated systems, which are essential for precision forestry, biodiversity monitoring, and the automation of forestry equipment. These tasks rely on advanced perceptual capabilities, such as detection and fine-grained species classification of individual trees. Yet, existing datasets are inadequate to develop such perception systems, as they often focus on urban settings or a limited number of species. To address this, we present SilvaScenes, a new dataset for instance segmentation of tree species from under-canopy images. Collected across five bioclimatic domains in Quebec, Canada, SilvaScenes features 1476 trees from 24 species with annotations from forestry experts. We demonstrate the relevance and challenging nature of our dataset by benchmarking modern deep learning approaches for instance segmentation. Our results show that, while tree segmentation is easy, with a top mean average precision (mAP) of 67.65%, species classification remains a significant challenge with an mAP of only 35.69%. Our dataset and source code will be available at this https URL."},{"url":"https://willguimont.com/blog/docker-workshop/","title":"Docker Workshop","body":"Docker and containerization have revolutionized the way applications are developed, deployed, and managed.\nContainers provide a lightweight and efficient way to package applications and their dependencies, ensuring consistency across different environments.\nSetup\nThe original workshop was designed to use with a virtual machine (willGuimont/IFT2001-Docker).\nWhile you can follow the instructions below to set up the virtual machine, you can also choose to follow the workshop using Docker.\nFollow these steps to prepare your environment.\nInstall Docker\ndocker run -it --rm ubuntu:latest bash\nIn the Docker container, run:\nsudo apt update &amp;&amp; sudo apt install -y git\nmkdir .atelier &amp;&amp; cd .atelier &amp;&amp; git clone https://github.com/willGuimont/IFT2001-Docker\ncd ateliers-iftglo-2001\n./helper/install.sh\npython3 ./helper/prepare_bashrc.py &amp;&amp; source ~/.bashrc\nCopy the files in your home directory: cd ~/.atelier/ateliers-iftglo-2001/files &amp;&amp; cp -r . ~/\nInstructions\nThe instruction above sets up your environment for the workshop.\nThe grading commands are of the form correction_nn, where nn is an integer representing the exercise number, for example correction_03 for exercise 3.\nInstructions for validating commands will be provided with the first question that has an expected result.\nTo complete this workshop, you must perform each exercise directly in the terminal.\nYou will use the nano text editor to modify the requested scripts.\nTo open a file with nano, run the following in the terminal.\nYou can then edit the file; enter\nTo exit nano, press CTRL-X, then answer y to save the changes.\nRun the following commands to validate that everything works:\nThese commands should print Hello world.\nAdventurers only\nIf you are comfortable with the command line, we encourage you to try completing this workshop using vim as your text editor.\nA brief introduction to vim commands is available on MIT’s The Missing Semester.\nvim offers many shortcuts to edit text and code very efficiently.\nAs a programmer, you will spend a lot of time writing code, so investing in learning vim is worthwhile for the rest of your career.\nThere are plugins emulating vim commands for most IDEs: Vim for VS Code or IdeaVim for JetBrains products.\nPlease note that no assistance regarding vim will be provided during this workshop.\nSolutions\nProposed solutions to the exercises in these workshops are available in the following GitHub repository: ulavalIFTGLOateliers/IFT2001-Docker.\nDocker\nAfter your success with Bash, your manager has given you a new mission: find a solution to the application deployment problem.\nYour company frequently encounters outages due to incompatibilities between different dependency versions.\nIndeed, each developer can choose the operating system they wish to work on.\nConsequently, some developers use different Linux distributions (Ubuntu, Arch, Gentoo), while others prefer Windows.\nThis situation often leads to version issues, as dependencies vary from one OS to another.\nAnd worse, the versions used differ from those deployed on the production server.\nThe recurring excuse “It worked on my machine” when a developer causes a production outage has finally exasperated your manager. He is asking you to find a solution.\nThat’s how you discover Docker, a platform that lets you build, deploy, and run applications in lightweight, isolated containers.\nWith Docker, you can create a specific container for each microservice, including all required libraries and dependencies.\nThis approach ensures that each application will run consistently, regardless of library versions used by individual developers.\nMoreover, Docker allows you to reproduce the development environment on the production server, thereby eliminating issues stemming from environment differences.\nWith Docker, you will tackle the challenge of running your company’s three main microservices.\nHere are the microservices you must get working:\na web server health monitoring application written in Haskell;\na document management API written in Rust, requiring a PostgreSQL database; and\na Python application for visualizing web server health.\nWhy containerization?\nIsolation\nOne of the main advantages of containerization is isolation.\nIt often happens that different applications require specific dependency versions.\nWith Docker, you can install different dependency versions in distinct containers, without risking conflicts or compromising other applications.\nFurthermore, if you need to run applications on different operating systems, such as Ubuntu and Arch Linux, you can simply wrap them in Docker containers, avoiding the need to rewrite applications for each system.\nPortability\nPortability is another key advantage of Docker.\nOnce you have set up a development environment with all required dependencies, reproducing that configuration on other machines or for new team members becomes tedious.\nThanks to Docker, you can create an image that contains all required dependencies and configuration, which can be easily distributed and run on different platforms, whether Linux, Windows, or others.\nThis enables a new team member to start quickly without spending weeks configuring their environment.\nIt’s also an advantage when deploying applications to a server.\nReproducibility\nReproducibility is also simplified with Docker.\nDocker images are versioned, which means it is easy to reproduce builds identically, ensuring that development, test, and production environments are consistent.\nYou just need to specify the version of the Docker image used to guarantee consistency and avoid issues related to variations across environments.\nEfficiency\nIn terms of efficiency, Docker offers a significant advantage over virtual machines (VMs).\nUnlike VMs, Docker does not need to run a full operating system for each container, which greatly reduces resource consumption.\nDocker containers share the host kernel, making them lightweight and quick to start.\nCommunity\nDocker’s surrounding community is very active and offers a multitude of preconfigured images for various popular tools and technologies such as Node.js, Python, and many others.\nThese preconfigured images facilitate deployment and usage of these technologies, saving developers time by avoiding manual environment setup.\nIndustry\nFinally, it’s important to emphasize that Docker has become a de facto standard in the software development industry.\nMany companies use Docker for development, testing, and deployment of their applications.\nDocker is now an essential tool for software developers.\nWhat is containerization?\nDocker is an open-source software platform that lets you create, deploy, and run applications in lightweight, isolated containers.\nA Docker container is a runtime unit that encapsulates an application together with all of its necessary elements, such as libraries, dependencies, and configuration files.\nThese containers are self-contained and portable, meaning they can run consistently across different systems, whether a development, test, or production environment.\nDocker is similar to a virtual machine that isolates an application from the host system.\nHowever, Docker is much more efficient than a VM, which requires a full operating system for each instance.\nDocker instead shares the host operating system’s kernel, saving resources and making containers faster to start and run.\nHere is some Docker terminology.\nYou do not have to read the advanced sections to complete this workshop.\nAs a supplement, you can watch the video “Never install locally”.\nDocker Image\nA Docker image is a template or build blueprint that contains all the elements needed to run an application.\nIt includes the operating system, libraries, dependencies, the application’s source code, and configuration files.\nDocker images are created from files called Dockerfile that specify the steps to build the image.\nDockerfile\nA Dockerfile is a text file that contains the instructions to build a Docker image.\nIt specifies the image layers, dependencies to install, files to include, and commands to run during image construction.\nDocker Container\nA Docker container is a running instance of a Docker image.\nIt is an isolated environment that runs the application with its dependencies.\nContainers are lightweight, portable, and self-contained, which makes them easy to deploy across different machines.\nDocker Registry\nA Docker registry is a centralized repository that stores and manages Docker images.\nThe default public registry is Docker Hub, where you can find many ready-to-use images.\nYou can also create and use your own private registry to store your own images.\nUnion filesystem (advanced concepts)\nThe Union Filesystem, also known as UnionFS or OverlayFS, is a technology used by Docker to manage images and container layers efficiently.\nThe Union Filesystem allows you to overlay multiple file systems into a single logical view without physically merging them.\nThis means Docker images and containers can share and reuse common file layers, saving storage space.\nIt also speeds up image builds by caching layers already built.\nWhen a container starts, a new read-write layer is added on top of the image layers, enabling container-specific changes without affecting others.\ncgroups (advanced concepts)\ncgroups, or control groups, are a Linux kernel feature used by Docker to limit and manage system resources used by containers.\ncgroups allow control of resources such as CPU, memory, disk bandwidth, and network, ensuring balanced and fair resource usage among containers.\nDocker uses cgroups to set limits and quotas on resources allocated to each container, ensuring isolation and predictable performance.\nNamespaces (advanced concepts)\nNamespaces are a Linux kernel feature that isolates system resources among processes.\nDocker uses several types of namespaces to provide isolation between containers, including the PID namespace (process isolation), the network namespace (network isolation), the user namespace (user isolation), and the mount namespace (mount point isolation).\nThese namespaces ensure that each container has its own isolated view of the system, preventing processes in one container from interfering with others or with the host system.\nChroot jail (advanced concepts)\nChroot, or change root, is a Unix/Linux feature that changes a process’s root directory and limits its access to the file system.\nDocker uses chroot to create an isolated environment inside the container, where the container’s root directory becomes the starting point for all file paths.\nThis limits the container’s access to files and directories outside its isolated environment, thereby strengthening security and isolation.\nContainers manually\nTo clearly illustrate what Docker does, we will manually build a simplified container without using Docker.\nThis section is inspired by p8952/bocker: Docker implemented in around 100 lines of bash.\nIn a terminal, enter the following commands:\nAs you can see, there is no magic in containers.\nWe use basic Linux tools to isolate a process in its own filesystem.\nDocker basics\nExercise 01 – Running a prebuilt image\nRun the Docker image hello-world.\nTo run a Docker image, use the run command.\nIf the command works, you will see a message displayed.\nSolution\nTo execute a command in the container, specify the command at the end of run.\nIn the following examples, we pass bash -c 'echo \"Hello world\"' which interprets the string in the Bash interpreter.\nTo launch an interactive command (such as a shell), use the -it flag.\nThe --rm flag removes the container once it exits.\nExercise 02 – Container management\nTo see running containers: docker ps.\nTo stop a container: docker stop container_id.\nNote that the command may take some time to run.\nTasks:\nLaunch an interactive terminal with Docker;\nOpen another terminal and inspect running containers;\nStop the Docker container from the second terminal.\nSolution\nExercise 03 – PostgreSQL database\nTo set environment variables in a container, use the -e flag.\nTo open a network port, use the -p docker:host flag, where docker is the port number inside the container and host is the port number on the host.\nLaunch a PostgreSQL database with Docker.\nRequirements:\nImage name: postgres\nSet the environment variable POSTGRES_PASSWORD=postgres\nMap Docker port 5432 to host port 5432\nLeave this container running.\nSolution\nDockerfile\nDockerfiles are configuration files used to create custom Docker images.\nThey allow you to define, reproducibly and automatically, an application’s runtime environment inside a Docker container.\nA Dockerfile contains a series of instructions that specify the steps needed to build a Docker image.\nThese instructions include actions such as selecting the base image, installing dependencies, configuring environment variables, copying files, running commands, and more.\nHere is an annotated example Dockerfile for a Python application.\nTo build the image, use docker build -t &lt;tag&gt; ..\nThe -t &lt;tag&gt; argument gives the image the name &lt;tag&gt;.\nFor the previous application, you could use docker build -t python-app ..\nNote that the order of instructions matters.\nIt is crucial to define the order of operations in a Dockerfile due to how Docker builds images.\nEach instruction in the Dockerfile creates a new layer in the Docker image, and the order can significantly impact build efficiency and performance.\nDocker uses a cache to speed up image builds.\nWhen you run an instruction, Docker checks whether that instruction has already been executed in a previous layer.\nIf so and the parameters are identical, Docker reuses the cached layer instead of rebuilding it.\nThis saves build time.\nHowever, if you modify an instruction higher up in the Dockerfile, all subsequent instructions will have their cache invalidated and must be rebuilt.\nFor example, if you perform resource-intensive operations such as compiling code that changes whenever you modify the source, it can be preferable to place those steps towards the end of the Dockerfile to benefit from caching as much as possible.\nExercise 04 – Haskell service Dockerfile\nUse the README.md in the folder ~/Applications/status-checker to write a Dockerfile that runs the Haskell application.\nRequirements:\nBase image haskell:9.0-buster\nRun stack setup --install-ghc\nWorkdir /app\nCopy the source code into /app\nRun stack build\nLaunch the server with stack run\nThen, start the container exposing port 8080.\nIf everything works, you should be able to run:\nLeave this container running.\nSolution\nDockerfile:\nExecution:\nExercise 05 – Python application Dockerfile\nUse the README.md in the folder ~/Applications/python_app to write a Dockerfile that runs the Python application.\nThen, launch the application interactively and with the correct network mode.\nThis application will need to access your computer’s local network to make requests to the container from the previous exercise; to do so, pass the argument --network=\"host\" when you run the container.\nIf everything works, you should be able to use the application to add URLs to monitor and see the results.\nSolution\nDockerfile:\nExecution:\nAdvanced Docker\nSometimes, you may need to create more complex Docker images.\nHere, we will explore two advanced Docker features: multi-stage builds and volumes.\nMulti-stage image builds\nMulti-stage builds are an advanced Docker feature that enables optimized image creation using several distinct stages in the build process.\nThis separates build and production stages, often resulting in final images that are lighter and more secure by minimizing what remains in the final image.\nFor example, for a JavaScript React application, we can split the build into two stages.\nCreate the application:\nDockerfile:\nTo run:\nExercise 06 – Rust multi-stage build\nUse the README.md in the folder ~/Applications/rust_api to write a Dockerfile that runs the application.\nRequirements:\nLaunch the PostgreSQL database from a previous exercise;\nCreate a first stage to compile the Rust code;\nCreate a second stage to run the code;\nStart this image from debian:bullseye-slim;\nInstall the package libpq-dev with apt update; apt install -y libpq-dev;\nSet the environment variable DATABASE_URL=postgres://postgres:postgres@localhost\nCopy /app/target/release/rust_api from the previous stage;\nRun the container with --network=\"host\" and expose the correct port;\nTest that everything works with curl http://localhost:8081/documents.\nSolution\nExecution\nVolumes\nIn Docker, volumes are used to allow containers to access, share, and persist data between the host system and the container itself.\nDocker volumes store data outside the lifecycle of containers.\nThis means that even if you destroy or recreate a container, the data stored in the volume remains intact.\nThis separates data persistence from the container environment, providing better data management.\nVolumes are also useful to grant containers access to datasets too large to copy into the image, such as a training dataset for a machine learning system.\nFor example, to store PostgreSQL database data in a host folder, specify the PGDATA variable and mount a volume:\nTo avoid rebuilding the Docker image after each change to the source code—and thus speed up development—you can make the project’s source code accessible via a volume.\nThus, a DockerfileDev for the Python application could look like this:\nRun with:\nThus, you can reuse the same image even if the source code has changed.\nExtras\nSome additional Docker features and tools that may be useful in your projects.\nCleanup\nWhen using Docker, it is important to consider the storage used by images, containers, volumes, and other Docker artifacts.\nPoor storage management can lead to excessive disk usage and make Docker maintenance and resource management difficult.\nDocker provides several prune commands to remove unused containers and images.\nTo clean up various Docker artifacts, the command docker system prune -a -f removes all unused resources.\nDocker Compose\nDocker Compose is a tool that allows you to easily define and manage multi-container applications.\nIt simplifies deployment and orchestration of Docker containers using a simple, readable configuration file.\nWith Docker Compose, you can specify services, networks, volumes, and other configurations needed to run an application composed of multiple containers.\nYou can also define dependencies among containers, environment variables, exposed ports, etc.\nDocker Compose uses a YAML configuration file to describe the application’s infrastructure.\nThis file contains sections such as services, networks, volumes, etc., where you can define the different parts of your application and their configurations.\nHere is an example docker-compose.yml for the Rust application that launches both the server and the PostgreSQL database:\nRun with:\nPodman\nPodman is an open-source alternative to Docker.\nDocker and Podman are two popular containerization tools that share similar features but differ in architecture and security approach.\nDocker uses a client-server architecture, where the Docker daemon runs as a separate process and Docker commands are executed via the CLI.\nIn contrast, Podman uses a daemonless architecture, meaning it runs directly as a regular user and does not require a separate daemon process.\nThus, Podman can run without special privileges, limiting potential risks associated with running as superuser. See What is podman? for more information.\nPodman also offers tools to manage pods, a topic outside the scope of this workshop.\nKubernetes\nKubernetes is an open-source container orchestration system that facilitates the deployment, management, and scaling of containerized applications.\nUsing Kubernetes offers many benefits, such as fault tolerance, easier deployments, and management and scaling of containerized applications."},{"url":"https://willguimont.com/blog/bash-workshop/","title":"Bash Workshop","body":"Bash is a powerful command-line interpreter that allows users to interact with their operating system through text commands.\nAs a programmer, you will often need to use the command line to perform various tasks, such as navigating the file system, managing files and directories, and executing scripts.\nSetup\nThe original workshop was designed to use with a virtual machine (willGuimont/IFT2001-Scripting).\nWhile you can follow the instructions below to set up the virtual machine, you can also choose to follow the workshop using Docker.\nFollow these steps to prepare your environment.\nInstall Docker\ndocker run -it --rm ubuntu:latest bash\nIn the Docker container, run:\nsudo apt update &amp;&amp; sudo apt install -y git\nmkdir .atelier &amp;&amp; cd .atelier &amp;&amp; git clone https://github.com/willGuimont/IFT2001-Scripting\ncd ateliers-iftglo-2001\n./helper/install.sh\npython3 ./helper/prepare_bashrc.py &amp;&amp; source ~/.bashrc\nCopy the files in your home directory: cd ~/.atelier/ateliers-iftglo-2001/files &amp;&amp; cp -r . ~/\nInstructions\nThe instruction above sets up your environment for the workshop.\nThe grading commands are of the form correction_nn, where nn is an integer representing the exercise number, for example correction_03 for exercise 3.\nInstructions for validating commands will be provided with the first question that has an expected result.\nTo complete this workshop, you must perform each exercise directly in the terminal.\nYou will use the nano text editor to modify the requested scripts.\nTo open a file with nano, run the following in the terminal.\nYou can then edit the file; enter\nTo exit nano, press CTRL-X, then answer y to save the changes.\nRun the following commands to validate that everything works:\nThese commands should print Hello world.\nAdventurers only\nIf you are comfortable with the command line, we encourage you to try completing this workshop using vim as your text editor.\nA brief introduction to vim commands is available on MIT’s The Missing Semester.\nvim offers many shortcuts to edit text and code very efficiently.\nAs a programmer, you will spend a lot of time writing code, so investing in learning vim is worthwhile for the rest of your career.\nThere are plugins emulating vim commands for most IDEs: Vim for VS Code or IdeaVim for JetBrains products.\nPlease note that no assistance regarding vim will be provided during this workshop.\nSolutions\nProposed solutions to the exercises in these workshops are available in the following GitHub repository: ulavalIFTGLOateliers/IFT2001-Scripting.\nIntroduction to Bash\nIt’s a day like any other at the office. You arrive early in the morning, coffee in hand and your laptop under your arm.\nAs you enter your department, you notice that your usually calm and relaxed boss is pacing nervously down the hallway. His pale expression and trembling hands immediately catch your attention.\n“Worrying — what’s going on?” you ask, concerned.\n“Production has crashed,” he replies in a trembling voice. “We’re getting hundreds of customer calls, nothing works anymore…”\nYour heart races as you realize the magnitude of the problem. Your company depends on a server that hosts most of its services, and it seems something is malfunctioning. Your boss looks you straight in the eye.\n“Solving this problem is your priority,” he says firmly.\nHe hands you a sheet of paper with the login credentials to access the server.\nYou feel both nervous and excited at the prospect of solving this critical issue.\nYour boss then leads you to the server room, where an old CRT screen and a keyboard await you.\nHe explains that it’s a Linux server edition, so without a graphical interface.\nYou will therefore have to solve the problems using only the terminal.\nThe command line and the manual\nOpen a terminal with the shortcut Ctrl+Alt+T, or press the Super key (the Windows key on the keyboard) and search for the Terminal application.\nAs a first step, you decide to inspect the files present on the server in order to find the log files. However, you are not sure which command to use for this.\nFortunately, the command line lets you learn how each command works. The man command allows you to read the manual page corresponding to a specific command.\nFor example, to get the documentation for the ls command, you can use the following command:\nOnce you have opened the manual page, you can navigate using the arrow keys to read the content.\nTo quit the manual page, simply press the q key.\nAlso, some commands accept the --help argument which displays a help message describing the arguments that can be passed to the command.\nFor example:\nBy running this command, you will get a help message detailing the different options and arguments you can use with the ls command.\nDo not hesitate to use the --help argument with commands you wish to explore to obtain additional information on their usage.\nThis can help you better understand the available features and correctly use commands in your exploration of the server.\nFor some commands such as cd (which is a command built into the Bash shell), you should instead use\nTerminal shortcuts\nAuto-completion: You can use the Tab key to get auto-completion in the terminal.\nQuit a command: In a terminal, the Ctrl+C shortcut, rather than copying, terminates the execution of a command.\nFor example, the yes command indefinitely repeats the letter y in the terminal.\nTo quit, press Ctrl+C:\nTo copy and paste, the terminal uses Ctrl+Shift+C and Ctrl+Shift+V\nTo close a terminal, you can use Ctrl+D.\nExercise 01 – Basic commands\nThis exercise aims to familiarize you with a few basic commands that will be useful throughout the workshop.\nTo do this, use the man command and the --help argument to obtain detailed information on how each command works.\nIn the file exercice_01.txt, briefly describe the purpose of each of the following commands.\nThis cheat sheet will be useful throughout the workshop.\nYou can consult it with the command cat exercice_01.txt.\nOpen exercice_01.txt with nano with:\nIn another terminal (Ctrl+Alt+T to open another terminal or Ctrl+Shift+T to open another terminal in a tab), use the terminal to determine the behavior of the following commands:\nCommands\nnano\ncd\nls\ncat\nmkdir\nrm\nrmdir\nmv\npwd\ncp\nchmod\ntouch\nSolution\nCommandDefinition\nnanoSimple text editor\ncdChange current directory\nlsList files in a directory\ncatDisplay a file’s content\nmkdirCreate a directory\nrmDelete a file or directory\nrmdirDelete an empty directory\nmvMove/rename a file\npwdShow the path of the current directory\ncpCopy a file\nchmodChange file permissions\ntouchUpdate a file’s timestamp, or create it if it does not exist\nExercise 02 – File system navigation\nUse the commands listed above to explore the directory tree and find the file with the .log extension.\nHere are some special paths:\n. represents the current directory;\n.. represents the parent directory;\n~ represents the user’s home directory (/home/username/);\n- represents the path of the last visited directory.\nYou can use these special paths with several commands, notably cd.\nList the files in the current directory\nMove through the different directories and try to find the file with the .log extension\nCopy the absolute path (from the root of the file system /) of the directory where the log files are located into the file ~/out_02.txt. Do not add a newline at the end.\nSolution\nCommands to run:\nls\ncd ApplicationData/output/logs\nls\npwd\nExpected path: /home/glo2001/ApplicationData/output/logs\nExercise 03 – home directory and copy\nNow that you have found the log file, it’s time to make a copy of it in your home directory.\nThe home directory is where a user’s personal files are stored.\nEach user has their own directory in /home/.\nIf your username is username, so your home directory is located at /home/username.\nThere is also a shortcut to refer to the home directory: ~. Thus, each of the following commands returns you to the home directory:\nCopy the .log file into the ~/log_backup directory.\nReturn to your home directory;\nCreate a new directory named log_backup in your home directory;\nCopy the .log file into the backup directory, giving it the name build_backup.log.\nSolution\nCommands:\ncd\nmkdir log_backup (typo in LaTeX was mdkdir but intent is mkdir)\ncp ~/ApplicationData/output/logs/build.log ~/log_backup/build_backup.log\nExercise 04 – Permissions\nNow that you have backed up the log file, it’s time to run the diagnostic script provided by your boss to analyze the system.\nGo back to the log directory, and try to run the script diagnostic.sh using ./diagnostic.sh.\nHowever, you encounter a permissions error.\nTo fix this, inspect the script permissions using the ls -l command.\nThen, modify the permissions to make the script executable.\nchmod, short for change mode, is a command used to modify access permissions for files and directories.\nPermissions in bash refer to the access rights granted to users and groups to read, write, and execute files and directories.\nThese permissions control who can perform which operations on a given file or directory.\nYou can list file and directory permissions with ls -l.\nPermissions are generally represented by one character followed by three groups of three characters each, for a total of nine characters, displayed in a specific order:\nThe first character indicates whether it is a file (-) or a directory (d).\nThe first group of three characters represents the permissions of the file’s owner.\nThe second group represents the permissions of the group to which the file belongs.\nThe third group represents the permissions for other users.\nEach group of three characters consists of the following permission types:\nr (read): Read the content of the file or directory.\nw (write): Modify or delete the file (or the contents of the directory).\nx (execute): Execute a file (or traverse a directory).\nTo modify these permissions, you can use the chmod command.\nFor example, to add (+) the execute permission to a file:\nTo remove the write permission from a directory:\nSteps:\nInspect the file permissions;\nUse chmod with the +x argument to modify permissions;\nRun diagnostic.sh.\nSolution\nExercise 05 – Wildcards\nThe script generated about ten .out files containing the results of the system analysis. You need to move these files into a new folder named output.\nInstead of moving each file manually with the command mv out_01.out output/, which would be tedious, you can use the wildcard character (*), which allows you to select multiple files at once.\nBefore moving the files, you can try the command cat *.\nThis command displays the contents of all files in the current directory.\nHowever, in your case, you do not want to select all files.\nYou can specify a specific pattern by adding a prefix or suffix to the filenames.\nFor example, to select all .sh files, you can use the command ls *.sh.\nThis will display the list of files with the .sh extension.\nTasks:\nCreate a folder named output_backup in the ~/ApplicationData/output/logs directory;\nMove all files ending with .out into the folder in a single command.\nSolution\nExercise 06 – Deleting files and folders\nIn addition to generating .out files, the script also generated temporary .tmp files and a folder named temp.\nThese files and this folder can be deleted.\nTasks:\nDelete the files with the .tmp extension;\nDelete the temp folder.\nSolution\nExercise 07 – Basic scripting\nTo simplify the task of moving and deleting the files generated by the diagnostic.sh script, you can create a script that will automate these actions for you.\nHere is an example Bash script:\nIn this script, the line #!/usr/bin/env bash is called a shebang (she = #, bang = !).\nIt tells the interpreter which program should be used to run the script—in this case, Bash.\nIt would also be possible to specify another program as the interpreter.\nFor example, to interpret the script as Python 3, we would use the following shebang: #!/usr/bin/env python3.\nUsing #!/usr/bin/env bash is generally preferable to #!/bin/bash, as it looks up the location of the Bash executable in the user’s environment, making it more portable across systems.\nYou can create a text file with the .sh extension (for example, example.sh), copy the script above into it, then make the file executable using chmod +x example.sh.\nNext, you can run the script using ./example.sh.\nNow, create a script that performs the following operations:\nCreate a script named cleanup.sh in the directory ~/ApplicationData/output/logs/;\nGive the script execute permissions;\nUse the commands you typed in the previous exercises to create your script:\nCreate a folder named output_backup;\nMove all files ending with .out into the folder in a single command;\nDelete the files with the .tmp extension;\nDelete the temp folder.\nMake sure to delete the output folder you created earlier;\nRun diagnostic.sh once more, and test your script.\nYou can use the history command to view the list of commands you have previously executed in the terminal.\nSolution\nAdvanced commands\nNow that you’ve performed some cleanup operations and become familiar with the command line, it’s time to analyze the logs and outputs of the diagnostic program.\nTo do this, you will need to combine several commands using piping and more advanced commands.\nBefore addressing this topic, you will explore some more advanced commands that will be useful later.\nExercise 08 – Advanced commands\nAs in exercise 1, use the man command and the --help argument to obtain detailed information on the commands in the following table.\nThese commands are a bit different from those seen in exercise 1; they can take a file as input, or you can pipe the output of another command to them, which will be the subject of the next section.\nYou can create a test file (test_file.txt) to test the commands.\nIn the file exercice_08.txt, briefly describe the purpose of each of the following commands.\nThis cheat sheet will be useful throughout the workshop.\nCommands\ntac\nless\nfind\ngrep\nsort\nuniq\nwc\nhead\ntail\ndu\ncurl\nsed\nawk\nkill\nsleep\nSolution\nCommandDefinition\ntacReverse the order of lines in a file\nlessText pagination\nfindFile search\ngrepFilter lines by a pattern\nsortSort lines\nuniqRemove duplicate lines\nwcCount lines, words, and characters\nheadDisplay the beginning of a file\ntailDisplay the end of a file\nduShow file sizes\ncurlNetwork request (HTTP, FTP, etc.)\nsedStream editor\nawkInterpreter for the awk language\nkillStop a process\nsleepWait for a number of seconds\nExercise 09 – Advanced commands 1\nThe messages.txt file in the log folder contains the error messages that cause the system outage.\nUnfortunately, this file also contains a lot of logs that are not useful to you.\nRather than manually reading the entire file, you decide to use the Bash commands you just discovered.\nUse a command to display all lines of messages.txt that contain the string Error and copy the result into the file ~/errors.txt.\nSolution\nExercise 10 – Advanced commands 2\nAfter inspecting the errors that you copied into ~/errors.txt, you realize there are many duplicates.\nUse a Bash command to remove duplicate lines and copy the result into ~/errors_2.txt.\nSolution\nExercise 11 – Advanced commands 3\nAfter inspecting the filtered errors from the file ~/errors_2.txt, replace errors with a 400 code (400, 403 and 404) with warnings.\nWith sed, replace the text using the regular expression 'Error \\(4[0-9]\\+\\) with the following text: Warning \\1 where \\1 will copy the number captured in the regular expression.\nThe command will have the form sed 's/regex1/regex2/g.\ns indicates that it is a substitution applied globally.\nUse a Bash command to modify the messages and copy the result into ~/errors_3.txt.\nSolution\nProgram composition\nProgram composition is at the heart of the Unix philosophy.\nHere is an excerpt from The Art of Unix Programming describing the importance of program composition (full chapter available here).\nIt’s hard to avoid programming overcomplicated monoliths if none of your programs can talk to each other.\nUnix tradition strongly encourages writing programs that read and write simple, textual, stream-oriented, device-independent formats. Under classic Unix, as many programs as possible are written as simple filters, which take a simple text stream on input and process it into another simple text stream on output.\nDespite popular mythology, this practice is favored not because Unix programmers hate graphical user interfaces. It’s because if you don’t write programs that accept and emit simple text streams, it’s much more difficult to hook the programs together.\nText streams are to Unix tools as messages are to objects in an object-oriented setting. The simplicity of the text-stream interface enforces the encapsulation of the tools. More elaborate forms of inter-process communication, such as remote procedure calls, show a tendency to involve programs with each others’ internals too much.\nTo make programs composable, make them independent. A program on one end of a text stream should care as little as possible about the program on the other end. It should be made easy to replace one end with a completely different implementation without disturbing the other.\n— Chapter 1. Philosophy, Rule of Composition\nThis program composition can be achieved using different operators, which will be the subject of the next sections.\nComposition\nThe ; operator allows you to run one command after another on the same line.\n(command1 ; command2)\nFor example, for exercise 7, we could rewrite the program as follows:\nThe &amp;&amp; operator runs the second command only if the first one succeeded (returns an exit code equal to zero).\n(command1 &amp;&amp; command2)\nThe || operator runs the second command only if the first one failed (returns a non-zero exit code).\n(command1 || command2)\nPipes\nThe | operator passes the output of one command as input to another.\nThis operator is also called a pipe.\n(command1 | command2)\nRedirection\nThe &gt; operator redirects a command’s output to a file, overwriting the file.\n(command &gt; file)\nFor example, it is possible to download a text file with curl:\nTo discard a command’s output:\nThe &lt; operator redirects a command’s input from a file.\n(command &lt; file)\nThe &gt;&gt; operator, like &gt;, redirects a command’s output to a file, but appends to the end of the file rather than overwriting it:\n(command &gt;&gt; file)\nThe &lt;&lt; operator is like &lt;, but allows you to pass multiple lines.\n(command &lt;&lt;delim [multiple lines] delim)\nExamples\nWe recommend trying these commands one at a time to fully understand each step of the pipeline, for example:\nExercise 12 – Piping and redirection\nUse piping and redirection to rewrite exercises 9, 10 and 11 as a single command.\nSolution\nExercise 13 – File sizes\nUse this command which returns the list of files and their size:\nEach line contains the size in bytes and the file name separated by a Tab character.\nUse the output of this command to find the five largest files. Your script must return five lines in the same format as du.\nSolution\nExercise 14 – Data analysis\nThe file ~/ApplicationData/db.tsv contains data in TSV format (TAB separated value).\nFormat:\niddatenametypesize\n1232023-12-25fooerror_log23\nUse this file to find the 10 rows with the smallest size (size column), keeping only the name column. You must keep the header of the retained column, i.e. the first line of ~/out_14.txt should be name.\nTo accomplish this task, you can use awk, a programming language specialized for text manipulation.\nFor example, to extract the 1st and 3rd columns, separated by a Tab:\nSolution\nScripting\nNow that you are more comfortable with Bash, it’s time to create more advanced scripts.\nThe following examples will help you understand how to create scripts that can take arguments, use variables, and implement control structures such as conditionals and loops.\nExercise 15 – Test script\nAfter all that cleaning and analysis, you decide it is wiser to just roll back to a previous version of the server until the problem is fixed.\nAs you are not sure which version is functional, you decide to write a script that will test the server and roll back to the previous version if the test fails.\nThe first step is to write a script that tests whether the server is functional.\nSave this script in the file ~/test.sh, and give it execute permissions.\nSolution\nExercise 16 – Version rollback script\nNow that you have a test script, you can write a script that will roll back to the previous version of the server if the test fails.\nUse the given revert_last_commit function to roll back to the previous version of the server.\nComplete the following script to roll back to a previous version of the server until the test script passes.\nSave this script in the file ~/revert.sh, and give it execute permissions.\nSolution\nExtras\n.bashrc file\nBelow are optional enhancements you can add to your ~/.bashrc (or ~/.bash_profile depending on your shell startup rules) to make day‑to‑day terminal work more pleasant.\nPrompt customization\nSimple minimal prompt (user@host:cwd$):\nPrompt with colors, Git branch and exit code indicator (status inlined, no helper function):\nRetro prompt:\nSafety &amp; quality of life options\nEnable some helpful shell options:\nFunctions\nPATH\nThe PATH environment variable contains a list of directories where the shell looks for executable files when you type a command.\nWhen you type a command, the shell searches through each directory in the PATH variable in order until it finds an executable file that matches the command name.\nThe first matching executable is executed.\nThe value of PATH is a colon-separated list of directories.\nAdding directories to your PATH allows you to run executables located in those directories without specifying their full path.\nFor example, if you have a script located in ~/.bin, you can add this directory to your PATH variable to run the script from anywhere.\nAlias git\nAs you will likely use git a lot, here are some useful aliases to add to your ~/.bashrc or ~/.gitconfig file.\nAliases allow you to create shortcuts for long commands.\nYou can add these aliases in your ~/.gitconfig file under the [alias] section.\nHere’s an example of useful git aliases from here:"},{"url":"https://willguimont.com/blog/random-photos/","title":"Random Photos I found on my Hard Drive","body":"Here are some pictures I took in somewhere in Quebec City."},{"url":"https://willguimont.com/blog/error-handling/","title":"Element of Errors Handling","body":"Errors are everywhere.\nAny computation, IO operation, database access, or API call – any of them can fail.\nDespite this, most programming languages either struggle to make error handling correct or make it painful.\nIt’s almost always easy to write the happy path.\nThe moment something goes wrong, though, most languages make the not-so-happy path either verbose, implicit, or dangerously ignorable.\nHandling errors well should be a first-class concern.\nIt feels as if errors are just an afterthought in mainstream languages, added inelegantly out of necessity.\nThis blog post stems from some experimentation on error handling I did back in 2021.\nAt the end of my undergraduate studies in software engineering, we were assigned a final project in collaboration with an industry partner.\nThey gave us carte blanche on the choice of technology.\nLooking to try less mainstream languages and explore new paradigms, we decided to explore and compare several less conventional stacks – paying particular attention to how each one approached error handling, since our project required high reliability and robustness.\nTrying to Catch Lightning in a Bottle Exceptions (Java and Friends)\nThe most well-known model is the try/catch mechanism.\nYou throw an error and hope someone, somewhere, catches it.\nOtherwise, your program crashes unceremoniously.\nIt’s an easy method for the error generating code, throw the exception and forget about it, but not simple.\nThis is similar to the distinction between simple and easy made by Rich Hickey in his talk “Simple Made Easy”.\nExceptions are complecting by nature, they intertwine normal control flow with error handling.\nThe control flow is no longer linear and predictable, you climb back up the stack looking for a handler.\nThey let you skip instructions, bubble errors up several layers, and bypass normal control flow.\nIn a way, they are sugar-coated gotos that jump through stack frames.\nYes, try/catch/finally blocks and RAII (in C++/Java with try-with-resources) can clean things up nicely (although, I prefer using defer or err_defer), but you’re still left with a system where it’s not obvious what code might throw.\nJava attempted to solve the uncertainty around which operations might throw exceptions through checked exceptions, but that just shifted the problem.\nYou now have to annotate everything, tracing all possible failure paths and punting the problem upstream.\nMost Java developers end up defaulting to unchecked exceptions anyway, sacrificing safety for sanity.\nEven Robert C. Martin said in Clean Code:\nChecked exceptions can sometimes be useful if you are writing a critical library: You must catch them.\nBut in general application development, the dependency costs outweigh the benefits.\n– Robert C. Martin. Clean Code. 2008.\nDespite their limitations, exceptions are still very much useful for their convenience and ease of use in simple cases.\nThat said, while it can be convenient to let domain exceptions bubble up – for instance, in a REST API handler where you want a 400 Bad Request – you’re still playing with possibly impossible-to-predict, and more importantly difficult to debug, control flow.\nThis is especially true in large codebases with many layers of abstraction, where an exception thrown deep in the stack can be caught and handled far away from its origin, making it hard to trace the error’s source.\nWhile you can try to limit this with coding standards and best practices, it remains challenging and cumbersome if the language does not provide good alternatives.\nGoing Somewhere with Errors (Go)\nGo brought back the C-style return code (i.e., non-zero return for errors), modernized as error values returned by the function, now with nil as the successful return value.\nYou call a function, and it returns both result and error.\nBy convention, the error is the last return value.\nThis makes error handling explicit, which is a good thing: you see where errors can occur and have to deal with them directly.\nYou can follow the code, and you should be able to know exactly what happens when an error is returned.\nYou can handle the error here and there, or return it to the function’s caller.\nBut this explicitness comes at the cost of verbosity and boilerplate.\nYou will see if err != nil peppered everywhere.\nAlso, Go gives you no tools to help with error handling.\nYou’re mostly on your own to handle errors consistently.\nGo does not provide any syntactic sugar for error handling, i.e., no try/catch, no pattern matching, no monadic operators.\nJust if err != nil over and over.\nThe little Go I did write felt like it required a lot of duplicate boilerplate code to handle errors properly.\nI had to keep if err != nil {} in my clipboard for quick pasting.\nYou end up either duplicating error propagation boilerplate or using helper functions to hide the verbosity (e.g., errors.Join) – at which point you’ve reinvented a monad poorly.\nTo its credit, Go includes one elegant feature for cleanup after an error might have occurred: defer.\nIt allows you to schedule resource cleanup (like closing files or releasing locks) regardless of how the function exits, which pairs well with manual error handling:\nZig\nZig handles errors a bit differently.\nInstead of adding an explicit error return value, Zig uses error union types.\nBy adding ! before a type, you indicate that the function can return either a value of that type or an error, e.g., fn doSomething() !i32.\nThus taking the union of the error set and the normal return type.\nYou can optionally narrow down the error set to specific errors prefixing the error type, e.g., fn doSomethingElse() ErrorType!ReturnType.\nIn addition, Zig has built-in syntax for propagating errors with try and catch.\nWhen you call a function that can return an error, you can use try to automatically propagate the error if it occurs, or get the value if it succeeds.\nWith catch, you can provide a default value or handle the error in place passing the error to a lambda.\nThis minimizes boilerplate while keeping error handling explicit.\nI quite like this approach, as it makes error propagation concise without losing clarity.\nZig also has defer for cleanup, similar to Go, but it adds errdefer.\nerrdefer is used when you want to clean up resources only if an error occurs.\nFor example, if you allocate a struct and an error happens later, you can use errdefer to free that struct only in the error case, and return the created struct on success.\nOdin\nI recently started experimenting with Odin, which has some interesting ideas around error handling.\nInstead of considering errors as exceptions or special types, Odin treats them as regular return values.\nAs it is the case for Go, Odin conventionally returns an error as the last return value of a function.\nUnlike Zig, which uses error unions that require declaring error types using error{}, Odin simply treats non-zero return values as errors.\nThis is paired with Odin’s commitment to make zero values useful defaults, so a function returning zero for an error indicates success.\nBut Odin goes further with built-in operators to streamline error handling.\nAssuming the last return value is the error, and that any non-zero value indicates an error, Odin provides or_else, or_return, or_continue, and or_break operators to handle errors concisely:\nor_else allows you to provide a default value if an error occurs.\nor_return will return from the current function if an error occurs.\nor_continue and or_break can be used in loops to skip iterations or exit loops on errors.\nOdin’s approach is reminiscent of Go’s explicit error handling but adds syntactic sugar to reduce boilerplate.\nFrom these errors as values methods, this is probably my favorite.\nIt builds on Go’s explicit error handling while providing operators like or_return to streamline common patterns.\nIt does not require separate error construction like Zig, making it conceptually simpler.\nOverall, Odin strikes a nice balance between explicitness and elegance in error handling.\nPlaying Tag with Errors (Rust)\nRust’s tagged unions Result&lt;T, E&gt; and Option&lt;T&gt; types offers a different approach.\nErrors are in the type system.\nThe compiler forces you to handle them - or explicitly ignore them (e.g., with unwrap, expect, etc.).\nYou can use Rust’s pattern matching to destructure and handle errors explicitly.\nRust also provides the ? operator for concise error propagation:\nThe ? operator propagates errors up, and makes code shorter, but it can also obscure control flow.\nThe whole flow of the function can be interrupted by a single ?.\nThis single symbol rewires control flow and introduces implicit short-circuiting, which can obscure the data path.\nAt least, the ? operator is limited to functions returning Result or Option, so its use is explicit in the function signature.\nThis is a limited form of monadic error handling.\nThis is arguably one of Rust’s strengths, it makes functional programming ideas more mainstream, e.g., algebraic data types, pattern matching, higher-order functions, and monadic error handling.\nOf course, Rust doesn’t force you to propagate or handle errors safely — you can always opt out:\nA Monadic Digression\nBefore writing this document, I assumed that Rust’s ? operator was limited to built-in types like Result&lt;T, E&gt; and Option&lt;T&gt;, effectively restricting its use to these specific monads.\nWhile this holds true on stable Rust, the nightly-only try_trait_v2 feature extends the language’s capabilities by allowing custom types to participate in ?-based control flow through the implementation of the Try and FromResidual traits.\nOutputs:\nIn this implementation:\nfrom_output is analogous to Haskell’s return (or pure in the applicative context), lifting a value into the monadic type.\nbranch corresponds to the monadic bind (&gt;&gt;=), determining whether to propagate the value or short-circuit.\nfrom_residual is required for integrating with other Try-compatible types and enabling error propagation across type boundaries.\nDespite this flexibility, Rust’s ? operator remains fundamentally tied to error-handling semantics.\nUnlike Haskell, where monads generalize sequencing of computations across various effects, Rust’s monadic ergonomics, through ?, are constrained to types modelling control flow interruption.\nDespite how tempting it might look to generalize ? to arbitrary monads, I would caution against it.\nUnlike Haskell, where monads are a first-class abstraction for sequencing computations with various effects, using ? with arbitrary monads could lead to confusion.\nA monad is just a monoid in the category of endofunctors, what’s the problem? (Haskell)\nWhere Rust brings functional error handling into a systems programming language – with Result&lt;T, E&gt;, Option&lt;T&gt;, and ? – Haskell has far more powerful abstractions.\nMonads give you composable effects, including errors.\nYou can throw, catch, pattern match, and compose computations in Either, Maybe, or more complex stacks using monad transformers.\nAs in Rust, Either Error Value encodes a computation that can fail.\nBut in Haskell, you can manipulate it using the full suite of monadic tools.\nThe example below shows how ExceptT—a monad transformer—extends any base monad (typically IO) with error-handling capabilities.\nIt generalizes the Either e a pattern, allowing you to compose error propagation with other effects (like IO, State, etc.).\nThis is elegant – you get typed, structured error handling that composes seamlessly with IO.\nBut once you start stacking more than one effect - say, ReaderT, StateT, and ExceptT – it quickly becomes hard to manage.\nLibraries like mtl, freer, or polysemy try to reduce this friction, but the conceptual weight remains high.\nThe learning curve is steep, and yes, monads still confuse people.\nNot because they’re inherently difficult, but because most programming education doesn’t equip you to think in algebraic structures.\nExcepting Monads\nDespite Haskell’s emphasis on pure functions and strong static typing, it still includes support for runtime exceptions.\nWhy? Because not all errors fit cleanly into a type-level model—especially when dealing with I/O or legacy code.\nHowever, these exceptions can be safely and idiomatically captured and transformed into more composable types like Either, making them compatible with the broader functional ecosystem.\nParsing and Non-Empty Chains (Scala)\nThis brings us to 2021, where this blog post really began.\nDuring my final undergraduate project with a company, we decided to step away from the mainstream and explore more functional tooling.\nThat led us to Scala, and more specifically, to Cats.\nIn many real-world domains – especially parsing, validation, and data ingestion – you don’t just want to know if something failed.\nYou want to know everything that failed.\nScala’s Cats library handles this elegantly with Validated and NonEmptyChain.\nUnlike Either, which short-circuits on the first failure, Validated accumulates all errors.\nOutputs:\nIf name and email both fail, you get both errors, not just the first.\nThat’s the kind of robustness you want in real-world parsing, form validation, or config loading.\nAnd all that without much boilerplate code.\nIt is important to note, though, that Validated is not a monad due to the accumulation of errors.\nEffect Systems (Haskell++)\nEffect systems bring to error handling – and to side effects more broadly – more powerful tools.\nRather than wrapping all effects in a monad like IO, these systems explicitly track them in the type signature.\nThis means you can determine exactly which effects a function might perform (I/O, logging, error handling, state, etc.) directly from its type – not by convention, but enforced by the compiler.\nThe polysemy library in Haskell is a good example of such a system.\nHere’s a simple example extracted from willGuimont/exercises_api.\nIt combines state, logging, and error handling, all visible in the type signature.\nThis makes exception handling more explicit and modular – you can define and run separate interpreters for each type of error, allowing each subsystem to handle its own failures independently.\nThis approach tickles my functional programming itch, but it comes with complexity.\nIt is very elegant of thinking of your program as a series of composable effects, that are then interpreted at the edges of your system.\nMore complex operations can be decomposed into smaller effects, before being interpreter.\nIn some ways, it feels like a sort of interpreter that compiles your source code into a simpler bytecode (core language primitive operations) that is then executed.\nHowever, the learning curve is steep, and the abstraction overhead can be significant for small to medium projects.\nTrust, but verify assert (D)\nSome languages, like Eiffel and D, support design by contract, a declarative way to specify preconditions, postconditions, and invariants.\nIf a contract is violated, the program crashes or throws, often with minimal recovery.\nThey are often even omitted when compiled in release mode.\nThus, they are not, as the other techniques overviewed in this document, ways to validate user input or alternatives to exceptions.\nDesign by contract is about catching programmer mistakes, not handling external failures.\nIf a function says “I expect a non-empty list,” and you give it an empty one, that’s not a runtime error to recover from – it’s a logic bug.\nThe contract makes that explicit.\nIn contrast, exceptions, Either, Result, and Validated are about handling real-world uncertainty: invalid input, missing files, network timeouts, and corrupted data.\nThese are expected failures that your program should handle gracefully.\nHere’s a simple example of contract programming using D:\nContracts in D – namely in, out, and invariant blocks - act like free unit tests embedded directly within your code.\nThey document and verify the intended behaviour of functions and invariants of types, automatically checked at runtime in debug builds.\nFor instance, a precondition on withdraw guarantees that the withdrawal amount is valid before the function runs, while a postcondition ensures the balance decreases.\nUnlike traditional unit tests, which are often separate and may omit corner cases, contracts enforce correctness systematically and immediately during development, catching logic errors as soon as they are introduced.\nThey don’t replace unit tests entirely, but they eliminate many boilerplate checks, serving as a robust safety net and specification tool during implementation.\nLet It Go Crash (Elixir)\nElixir (and its Erlang foundations) embraces a fundamentally different philosophy: don’t prevent all errors - expect them, isolate them, and recover from them.\nInstead of striving for defensive, error-free code at every level, Elixir leverages lightweight, isolated processes and robust supervision trees.\nEach process operates independently.\nWhen a failure occurs, it doesn’t propagate through shared memory or unwind a global stack.\nThe process simply crashes – and a supervisor decides what to do next.\nHere, errors are also values, represented as tagged tuples {:ok, value} and {:error, reason}.\nFunctions return these tuples, and you pattern match on them to handle success and failure cases explicitly.\nThis design makes fault tolerance an architectural feature, not an implementation detail.\nPattern matching ensures that error handling remains explicit and readable.\nElixir’s paradigm forces you to think about the not-so-happy path from the beginning.\nEvery function that returns :ok or :error reminds you that failure is part of the domain.\nYou can’t ignore it – you have to model it.\nUnlike Go, where the programmer is responsible for inspecting and propagating every error manually, Elixir encourages you to crash early and let the system self-heal.\nResilience is not patched in – it’s built in.\nConclusion: Errors Are the Norm, Not the Exception\nThere is no perfect model.\nBut there are better tradeoffs.\nEvery meaningful computation—every I/O operation, API call, database query – can fail.\nYet most programming languages treat error handling as a second-class concern: either overly verbose, dangerously ignorable, or both.\nThe happy path is (almost) always easy to write.\nThe hard part is everything else.\nThere’s no silver bullet.\nBut there are better tradeoffs – depending on your constraints, goals, and team culture.\nPersonally, my thoughts on error handling have evolved over time.\nIn the past, I leaned towards leveraging the type system to make errors explicit, like Haskell and Rust.\nBut lately, I’ve come to appreciate the simplicity of Odin’s approach, which balances explicitness with conciseness and Elixir’s philosophy of embracing failure as a first-class concern rather than defensive coding.\nErrors are the norm, not the exception.\nThe best systems are those that make handling them not only simple, but obvious."},{"url":"https://willguimont.com/blog/dumb2bar-re/","title":"Turn Your Dumbbells Into Barbell Plates","body":"Introducing Dumb2Bar — a 3D-printable adapter that transforms your dumbbells into barbell plates.\nNo need to buy separate weight plates — just reuse what you already own.\nPrint it. Bolt it. Lift.\n🔗 Get it on Cults3D:\nhttps://cults3d.com/en/3d-model/various/dumbell-to-barbell-adapter"},{"url":"https://willguimont.com/blog/cube/","title":"3D-Printed AprilTags That Withstand the Real World","body":"In many robotics projects, AprilTags are indispensable. But the standard approach — printing on paper, gluing to cardboard — doesn’t hold up outside the lab.\nTags peel, warp, fade, and disintegrate when exposed to handling, humidity, or rough environments.\nTracker3D.jl solves this.\nIt generates 3D-printable AprilTags engineered for the real world: flat, durable, weather-resistant, and easy to integrate directly into enclosures or mounts.\nWhether you’re field-testing robots or deploying long-term vision systems, these tags won’t fail you.\nWhy Tracker3D.jl?\n🚫 No more flimsy paper or fragile adhesives\n✅ Always flat, robust, and easy to embed\n☁️ Weatherproof and reusable\n🧩 Ideal for industrial, outdoor, or educational setups\nGitHub: willGuimont/Tracker3D.jl\nReady-to-print tags: Thingiverse"},{"url":"https://willguimont.com/blog/citizen-sciences/","title":"Using Citizen Science Data for UAV Image Analysis","body":"Co-authored paper in MDPI: Using Citizen Science Data as Pre-Training for Semantic Segmentation of High-Resolution UAV Images for Natural Forests Post-Disturbance Assessment\nThe ability to monitor forest areas after disturbances is key to ensure their regrowth. Problematic situations that are detected can then be addressed with targeted regeneration efforts. However, achieving this with automated photo interpretation is problematic, as training such systems requires large amounts of labeled data. To this effect, we leverage citizen science data (iNaturalist) to alleviate this issue. More precisely, we seek to generate pre-training data from a classifier trained on selected exemplars. This is accomplished by using a moving-window approach on carefully gathered low-altitude images with an Unmanned Aerial Vehicle (UAV), WilDReF-Q (Wild Drone Regrowth Forest—Quebec) dataset, to generate high-quality pseudo-labels. To generate accurate pseudo-labels, the predictions of our classifier for each window are integrated using a majority voting approach. Our results indicate that pre-training a semantic segmentation network on over 140,000 auto-labeled images yields an $F1$ score of 43.74% over 24 different classes, on a separate ground truth dataset. In comparison, using only labeled images yields a score of 32.45%, while fine-tuning the pre-trained network only yields marginal improvements (46.76%). Importantly, we demonstrate that our approach is able to benefit from more unlabeled images, opening the door for learning at scale. We also optimized the hyperparameters for pseudo-labeling, including the number of predictions assigned to each pixel in the majority voting process. Overall, this demonstrates that an auto-labeling approach can greatly reduce the development cost of plant identification in regeneration regions, based on UAV imagery."},{"url":"https://willguimont.com/blog/3ds-scanning/","title":"Free 3D Scanning using iOS devices","body":"The current state of 3D scanning apps on the App Store is a bit disappointing.\nMost of them are either paid or have a subscription model.\nNot doing enough scanning to justify the cost, I decided to look for a free alternative.\nI was pleasantly surprised to find that Apple’s own RealityKit framework has a sample app for exactly this purpose.\nScanning objects using Object Capture is a free app that lets you scan objects using your iOS device.\nTo get started, install Xcode and download the sample app from the Object Capture sample code.\nBuild and run the app on your iOS device, and follow the instructions to scan an object.\nOnce done, you can export the scanned object as a USDZ file or save all the images to your device.\nThe images can then be used with Building an object reconstruction app to create a high-quality 3D model.\nSince USDZ is not a widely supported format in the 3D printing world or CAD software, you can convert it to other formats using tools like Blender."},{"url":"https://willguimont.com/blog/dumb2bar/","title":"Dumbbell to Barbell Adapter","body":"This dumbbell-to-barbell adapter allows you to transform your dumbbells into barbell plates. Designed to fit on a standard barbell, this adapter ensures your dumbbells stay in place during even the most intense workouts. It’s a cost-effective, space-saving solution for those looking to expand their weightlifting capabilities without purchasing additional equipment.\nThe adapter fits on standard barbells with a diameter of 1“ (25.4 mm) and is compatible with dumbbells that have a handle length of 6“ (124 mm) or longer.\nAvailable here: Dumbbell to Barbell Adapter @ cults3d.com"},{"url":"https://willguimont.com/blog/borealtc/","title":"Terrain Classification for Boreal Forests","body":"Co-authored paper at IROS 2024: Proprioception Is All You Need: Terrain Classification for Boreal Forests\nRecent works in field robotics highlighted the importance of resiliency against different types of terrains. Boreal forests, in particular, are home to many mobility-impeding terrains that should be considered for off-road autonomous navigation. Also, being one of the largest land biomes on Earth, boreal forests are an area where autonomous vehicles are expected to become increasingly common. In this paper, we address the issue of classifying boreal terrains by introducing BorealTC, a publicly available dataset for proprioceptive-based terrain classification (TC). Recorded with a Husky A200, our dataset contains 116 min of Inertial Measurement Unit (IMU), motor current, and wheel odometry data, focusing on typical boreal forest terrains, notably snow, ice, and silty loam. Combining our dataset with another dataset from the literature, we evaluate both a Convolutional Neural Network (CNN) and the novel state space model (SSM)-based Mamba architecture on a TC task. We show that while CNN outperforms Mamba on each separate dataset, Mamba achieves greater accuracy when trained on a combination of both. In addition, we demonstrate that Mamba’s learning capacity is greater than a CNN for increasing amounts of data. We show that the combination of two TC datasets yields a latent space that can be interpreted with the properties of the terrains. We also discuss the implications of merging datasets on classification. Our source code and dataset are publicly available online."},{"url":"https://willguimont.com/blog/obsidian-broken-graph/","title":"Fix Obsidian Broken Graph View","body":"If the Graph View in Obsidian is broken after an update, you can fix it by following these steps:"},{"url":"https://willguimont.com/blog/citeshortcut/","title":"Cite website Bookmarklet","body":"To automatically cite a website in bibme.org, add the following bookmarklet to your bookmarks bar:\nClicking the bookmarklet will open the current page in bibme.org’s citation generator."},{"url":"https://willguimont.com/blog/det-bench/","title":"Benchmarking of Real-Time Object Detection Models","body":"Published a paper to ArXiv: Replication Study and Benchmarking of Real-Time Object Detection Models.\nThis work examines the reproducibility and benchmarking of state-of-the-art real-time object detection models. As object detection models are often used in real-world contexts, such as robotics, where inference time is paramount, simply measuring models’ accuracy is not enough to compare them. We thus compare a large variety of object detection models’ accuracy and inference speed on multiple graphics cards. In addition to this large benchmarking attempt, we also reproduce the following models from scratch using PyTorch on the MS COCO 2017 dataset: DETR, RTMDet, ViTDet and YOLOv7. More importantly, we propose a unified training and evaluation pipeline, based on MMDetection’s features, to better compare models. Our implementation of DETR and ViTDet could not achieve accuracy or speed performances comparable to what is declared in the original papers. On the other hand, reproduced RTMDet and YOLOv7 could match such performances. Studied papers are also found to be generally lacking for reproducibility purposes. As for MMDetection pretrained models, speed performances are severely reduced with limited computing resources (larger, more accurate models even more so). Moreover, results exhibit a strong trade-off between accuracy and speed, prevailed by anchor-free models - notably RTMDet or YOLOx models. The code used is this paper and all the experiments is available in the repository at this https URL."},{"url":"https://willguimont.com/blog/randaugment/","title":"RandAugment","body":"Presentation made for the course GIF-7010: Avancées en apprentissage automatique at Université Laval on RandAugment.\nHere are the slides:\nRandAugment.pptx\nRandAugment.pdf"},{"url":"https://willguimont.com/blog/obsidian-android-alternative/","title":"Sync Obsidian vaults to Android using Git","body":"Updated way to sync your Obsidian vault to an Android phone using Git.\nPrevious method can be found here.\nHere’s the procedure to sync your Obsidian vault to an Android phone.\nFirst, you’ll have to run these commands on your computer:\nCreate a SSH key using ssh-keygen -f ~/obsidian-phone-key -t ed25519 -C \"your_email@example.com\"\nAdd the public key (~/obsidian-phone-key.pub) to your GitHub account\nConnect your phone to your computer and allow the computer to access phone data (e.g., using LocalSend)\nCopy the private key (~/obsidian-phone-key) to your phone\nNow, on your phone:\nInstall F-Droid\nOpen F-Droid and install Termux\nIn Termux, install git using pkg install git\nIn Termux install termu-am using pkg install termux-am\nMount the phone storage using termux-setup-storage\nCopy your SSH key into ~/.ssh/ using cp ~/storage/shared/path/to/obsidian-phone-key ~/.ssh/\nFollow these instructions to add the ssh key\nNow go to the location you want to clone the repository (should be in ~/storage/shared/ for Obsidian to have access) and clone it using git clone git@github.com:username/repo.git\nYou can now open Obsidian\nOpen folder as vault\nBrowse to the repository you copied from your computer\nOpen F-Droid and install MGit\nFrom the settings page of MGit, import the SSH key\nSSH Keys\nDownload button in the upper right corner\nSelect obsidian-phone-key you copied from your computer\nGo back to the main menu of MGit, press the three dots in the upper right and press the Import Repository button\nBrowse to the repository you copied from your computer\nYou should now be able to pull from MGit\nYour Obsidian vault is now synced using Git!\n(Optional) If you want to also be able to manage the repository from Termux, run the command git config --global --add safe.directory ~/storage/shared/path/to/repo"},{"url":"https://willguimont.com/blog/segment-anything/","title":"Segment Anything","body":"Presentation made for the course GIF-7010: Avancées en apprentissage automatique at Université Laval on Segment Anything.\nHere are the slides:\nSegment Anything.pptx\nSegment Anything.pdf"},{"url":"https://willguimont.com/blog/wayback/","title":"Wayback Machine Bookmarklet","body":"To automatically open a website in the Wayback Machine, add the following bookmarklet to your bookmarks bar:\nClicking the bookmarklet will open the current page in the Wayback Machine."},{"url":"https://willguimont.com/blog/phd-proposal-passed/","title":"Ph.D. Proposal Passed","body":"Passed the Ph.D. proposal exam!\nPh.D. Proposal Final Version"},{"url":"https://willguimont.com/blog/phd-proposal/","title":"Ph.D. Proposal - A First Draft","body":"This is my Ph.D. proposal. It is a work in progress: Ph.D. Proposal"},{"url":"https://willguimont.com/blog/research/","title":"Random quotes about research","body":"A small collection of random quotes about research.\nResearch is what I’m doing when I don’t know what I’m doing.\n– Wernher von Braun\nIf it works in theory, it also works in practice. In theory.\nAnd some quotes from discussions with friends.\nResearch is like wandering through a maze in search of an knowledge. But you can’t see the walls, and you don’t know if you’re lost. So you just keep bumping into walls until you find the something.\n– A friend of mine\nWhere’s the box?\nMe: Let’s think outside of the box to solve this.\nA friend: But there is no box to think outside of.\nAnd a poem joke about this video Socialism Is When The Government Does Stuff\nScience is when you know stuff.\nAnd it’s more sciencey the more stuff you know.\nAnd if you know a real lot of stuff, it’s a paper.\n– A friend of mine"},{"url":"https://willguimont.com/blog/rousseau-sartre/","title":"La liberté de l'être humain selon Rousseau et Sartre","body":"Ceci est un essai que j’ai écrit pour le cours Philosophie - L’être humain au Cégep Limoilou, lors de mes études en sciences de la nature en 2016 au Cégep Limoilou.\nAyant par la suite étudié en ingénierie logicielle, j’ai eu peu d’occasions de réaliser des travaux de rédaction en philosophie.\nC’est donc un exercice assez intéressant que de relire ce texte et de le mettre en ligne.\nLe texte contient quelques maladresses stylistiques, témoignant de mon niveau de français à l’âge de 18 ans, mais je n’ai pas voulu le modifier pour garder une trace de mon évolution.\nIntroduction\nLa liberté. L’histoire contemporaine est entachée de guerres civiles dans le but de l’atteindre. Devant de telles évidences, il devient évident que la liberté est un enjeu primordial pour une majorité d’êtres humains. Depuis l’antiquité, cette facette de l’être humain est étudiée et discutée dans toutes les civilisations humaines. Avant tout, il est important de définir la liberté. Selon le dictionnaire Larousse en ligne, la liberté est une «&nbsp;Situation de quelqu’un qui se détermine en dehors de toute pression extérieure ou de tout préjugé&nbsp;». Cette définition pose plusieurs questions&nbsp;: l’être humain est-il libre ? Quels sont les éléments pouvant entraver notre liberté ? Plusieurs auteurs ont fait de la liberté une pierre angulaire de leur conception de l’être humain. Ainsi pour Rousseau et Sartre l’homme est avant tout un être de liberté.\nChapitre 1 - Époque des auteurs\nLes différentes luttes de pouvoirs ont des répercussions sensibles sur toute la société. Les travaux philosophiques représentent bien ces évènements puisque les auteurs s’inspirent directement des problèmes et avantages de son temps. Les époques de Rousseau et de Sartre sont intéressantes à comparer puisqu’elles représentent l’avant et l’après de la société par rapport à la Révolution française. Rousseau vivait à une époque de lutte de pouvoir contre la monarchie en France alors que Sartre vécut une France, certes en république, mais en guerre. Ces différentes époques ont grandement influencé ces auteurs ainsi que leur conception de l’être humain.\nRousseau et les Lumières\nRousseau né à Genève, le 28 juin 1712, au tout début de la période historique que nous appelons aujourd’hui le siècle des Lumières. Cette période de grandes révolutions influença profondément la conception de l’être humain de Rousseau. En effet, c’est à cette période que de grands philosophes tels Locke, Hume et Montesquieu critiquèrent le système de monarchie absolue jusqu’alors en place. Chacun proposa sa propre façon d’étancher la soif de liberté et d’égalité du peuple. S’inspirant de ces travaux contre la monarchie, Rousseau propose alors le concept de «&nbsp;contrat social&nbsp;» où tous se doivent d’être libres et égaux. Ces idées de libertés, d’égalité et de tolérance firent de Rousseau un héros de la Révolution française de mai 1789. Le mouvement de pensée des Lumières porte 3 idées en essence&nbsp;: l’esprit critique, la liberté et la philosophie. Ce siècle de progrès dans tous les domaines représentait parfaitement ces concepts de nouveauté. Pourtant, contre les courants de son époque, Rousseau reste très prudent quant au progrès fulgurant de l’humanité dans les différentes sphères de l’activité humaine, car il croit que le progrès ne participe pas au développement moral ni au bonheur de l’homme, mais plutôt à la promotion du paraître. Aussi, la découverte des Amériques et l’étude des Amérindiens éclairèrent un lien des plus naturels entre l’homme et la nature.\nAprès une enfance relativement comblée d’abord avec son père, puis chez le pasteur Lambercier, il est rapidement confronté à la rude discipline de son maître greffier qu’il considère comme cruel. Cette expérience juvénile avec l’autorité abusive peut lui avoir inspiré des années plus tard le principe de l’état de société.\nSartre et le XXe siècle\nSartre né à Paris, le 21 juin 1905, au début du XXe siècle. Période à laquelle se déroulèrent les deux Guerres mondiales. Ce siècle de grands bouleversements va grandement influencer Sartre ainsi que sa conception existentialiste de l’être humain. En effet, les conflits que vit l’Europe génèrent une grande inquiétude. Toute l’Europe est plongée dans une misère. La Première Guerre mondiale, le krach boursier de 1929 et la Seconde Guerre mondiale furent la cause de nombreux désespoirs. Sartre traduit bien l’incertitude de son époque. La monté d’un régime nazi ainsi que la Shoah rappelle bien que la vie est étrange. L’humanité est en proie à une barbarie sans précédent. Les Américains larguent des bombes atomiques d’une puissance jamais vue. Cette suite rapide d’évènement que fut le XXe siècle forgera les idées de liberté, d’égalité et des droits de l’homme de Sartre qui au cours de sa vie en devient un véritable défenseur. Sartre et son existentialisme humaniste ne vont pas sans rappeler l’humanisme du siècle des Lumières. Les crimes contre l’humanité, comme la Shoah, qu’ont commis les soldats sous les ordres de leurs supérieurs ont probablement influencé Sartre quant à la mauvaise foi&nbsp;: ces soldats rejetaient leurs responsabilités en disant qu’ils avaient reçu des ordres. Alors qu’il s’agissait totalement de leur choix de commettre ou non ces actes.\nChapitre 2 - Les conceptions de l’être humain\nRousseau\nSelon la conception naturaliste de l’être humain de Rousseau, l’être humain possède deux états&nbsp;: l’état de nature et l’état de société. L’état de nature représente l’état hypothétique qu’aurait vécu l’homme avant toute forme de société. Cet état d’isolement est caractérisé par la liberté, la perfectibilité, l’amour de soi, la pitié et l’égalité. L’homme est libre de nature parce qu’il peut choisir de ne pas suivre son instinct comme les autres animaux. La liberté est donc pour l’homme son don naturel. Forcés par leurs faiblesses individuelles face à la nature, les hommes se regroupent et forment des sociétés. Cet état de société impose plusieurs maux aux êtres humains&nbsp;: il y a dorénavant de la concurrence, on développe l’amour-propre et progressivement l’instauration d’une autorité abusant les plus pauvres. Bref, Rousseau postule que l’homme n’est de nature ni bonne ni mauvaise, mais que c’est la société qui le corrompt. Pour Rousseau, la solution aux maux de l’état de société est le contrat social par lequel l’homme devient citoyen et renonce à ses intérêts individuels pour participer à la volonté générale et profiter de liberté et d’égalité.\nSartre\nPour Sartre, l’homme est un être qui se construit par ses actes qu’il commet pour atteindre son but. L’homme se choisit en agissant de telle ou telle façon. En d’autres mots, l’homme est un projet qui s’oriente vers ce qu’il choisit d’être, vers son but. Ainsi, la liberté est de se construire se choisissant dans telle ou telle situation. La liberté est absolue, mais n’existe qu’en situation puisqu’on ne peut exercer notre liberté hors situation. Sartre oppose deux concepts opposés pour aider décrire l’homme&nbsp;: l’en-soi et le pour-soi. L’en-soi caractérise tout objet dépourvu de conscience qui ne peut être plus que ce qu’il est. Pour l’en-soi, l’essence précède l’existence&nbsp;: on conçoit l’objet avant de le réaliser, on ne le réalise pas sans savoir son utilité. Avant même d’exister, l’objet possède déjà une finalité et une essence. Bref, l’en-soi est le monde des choses, il est statique et fixe&nbsp;: toute chose possède une essence fixe. Sartre oppose ce concept à la notion de pour-soi, elle décrit l’homme qui se différencie de l’en-soi par sa conscience. Contrairement à l’en-soi, l’homme ne possède pas d’essence, il est donc ouvert à toutes possibilités. Pour l’homme, l’existence précède l’essence. Il n’est pas défini avant d’exister, nous ne sommes pas étiquetés avocat ou médecin, nous choisissons de le devenir après avoir été propulsé dans l’existence. L’homme est donc néant puisqu’il est la négation de l’en-soi et des divers déterminismes qui agissent sur lui. L’homme par sa conscience est totalement libre, mais il ne peut exercer sa liberté qu’en situation, c’est-à-dire dans des circonstances auxquelles l’homme donne un sens avec ses buts. L’homme se construit par ses actes. Il n’y a pas de liberté sans angoisse, puisque l’homme est le seul à pouvoir décider pour lui-même&nbsp;: il possède l’entière responsabilité de sa vie. Devant de telles libertés, l’homme peut fuir dans la mauvaise foi qui consiste à refuser de choisir, d’être responsable de ses actes ou de se montrer tel que l’on est. Pour un autre l’homme, l’être-pour-soi de l’un est l’être pour-autrui de l’autre. La conscience en plus d’être subjective est intersubjective. Sa conscience est aussi une conscience de celle des autres. L’homme vit dans un monde rempli de significations qu’il ne peut choisir. Son prochain lui donnera une signification qu’il ne peut choisir. De plus, les hommes avant lui auront déjà rempli le monde de significations motivé par leurs fins. Ce qui n’empêche pas l’homme d‘exercer sa liberté, puisque ces éléments ne sont qu’une situation comme une autre et c’est à l’homme de décider du sens de cette situation par rapport à son projet.\nChapitre 3 - L’homme est-il libre?\nRousseau\nSelon la conception naturaliste de l’être humain de Rousseau, l’homme est libre dans son état de nature. Dès les premières lignes du premier chapitre du premier livre Du contrat social, Rousseau pose cette idée en écrivant&nbsp;: «&nbsp;L’homme est né libre&nbsp;» 1. Rousseau écrit que la liberté représente donc un don de la nature que chaque être humain possède dès la naissance. La liberté devient alors un élément essentiel à l’homme. Sa capacité de choisir d’obéir ou non à son instinct animal fait partie de la définition de l’homme, de ce qui le différentie des bêtes. L’homme est donc libre de nature et constitue un droit que chacun se doit de pouvoir exercer.\nPourtant, selon Rousseau, il en est autrement en société. En se regroupant, l’homme est entré dans l’état de société qui brime sa liberté naturelle. Les autres peuvent empêcher un homme d’exercer sa liberté. Il y a alors compétition entre les individus et hiérarchisation. Cette hiérarchisation cause une inégalité sociale qui se traduit directement par une perte de liberté des oppressés qui pour profiter des avantages de la société (sécurité, un certain confort, etc.) doivent renoncer à leur droit à la liberté. Les gens puissants et riches abusent les hommes, les privant de leur liberté. Les autres en société représentent donc une entrave à la liberté individuelle puisqu’il y a domination et sujétion.\nS’il n’est pas possible de retourner à l’état de nature, les hommes peuvent participer à une association plutôt qu’à une domination. Les individus deviennent citoyens et participe de leur vouloir à un contrat social. Ce contrat en échange de la liberté et de l’égalité de tous demande aux individus qui veulent y participer de renoncer à leurs intérêts individuels pour embrasser la volonté générale. En suivant les lois, qui représentent l’intérêt général, ces dernières garantissent aux citoyens leur liberté.\nBref, selon Rousseau, l’homme est seulement libre dans son état hypothétique de nature. État qui avec la formation des premières sociétés fut remplacé par l’état de société qui infligea de nombreux maux aux hommes en les privant de leur liberté qui leur revient de droit. Rousseau propose alors un contrat social transformant l’homme en citoyen, lui rendant sa liberté qui lui revient de droit tout en lui procurant la sécurité que procure la société.\nSartre\nSelon la conception existentialiste de l’homme de Sartre, l’homme est libre, même «&nbsp;condamné à être libre&nbsp;» 2 puisque contrairement à l’en-soi, l’être humain doit se définir par ses actes. Il est responsable de son propre projet. N’ayant pas d’essence propre, l’homme est obligé de se construire en se choisissant. Ainsi, la liberté consiste à se choisir, se construire, à travers les situations que nous vivons. Toutefois, cette liberté ne prend sens qu’en situation, on ait le choix de donner tel ou tel sens à une situation en les comparants à ses buts. Cette constante remise en question amène de l’angoisse.\nJustement, cette angoisse peut pousser l’homme dans la mauvaise foi. Il agit comme s’il n’avait pas le choix, comme s’il n’était pas libre, alors qu’il l’est. C’est-à-dire qu’il refuse de choisir, qu’il refuse de se montrer tel qu’il est et qu’il refuse d’être responsable de ses actes. Un homme peut rejeter la faute sur autrui ou divers déterminismes, pourtant c’est lui qui a commis le geste&nbsp;: il se ment à lui-même et trompe les autres. Ainsi, un criminel pourrait plaider non-coupable et rejeter la faute sur son éducation, son milieu ou son entourage, pourtant, selon Sartre, l’homme est pleinement responsable de ses actes. Il essaie de se convaincre le jury que s’il a commis un crime, c’est qu’il n’avait pas le choix et que ce n’était pas de sa faute. Aussi, refusera-t-il de se montrer responsable de ses actes. Un homme peut aussi cacher sa propre nature en jouant un rôle&nbsp;: il n’agira plus en tant qu’être humain libre, mais en ce que l’on s’attend de lui. Sartre donne lui-même l’exemple d’un serveur qui agit comme s’il était destiné à être un serveur. Il agit par mauvaise foi parce qu’il cache sa propre vérité et trompe les autres&nbsp;: il n’agit pas comme l’être humain libre qu’il est, il agit comme s’il était serveur par essence. Dernièrement, refuser de choisir est de la mauvaise foi parce que la personne fuit la situation qu’elle vit. Elle choisit de ne pas choisir certes, mais elle fait semblant de ne pas avoir d’influence sur sa liberté. En quelque sorte, la mauvaise foi est une sorte d’entrave «&nbsp;psychologique&nbsp;» à la liberté, puisque l’on se fait croire que nous ne sommes pas libres alors que nous le sommes absolument.\nLa liberté est une liberté absolue, mais elle n’existe qu’en situation. On ne peut exercer sa liberté hors situation. Elle est en quelque sorte relative aux évènements que l’on vit. C’est-à-dire que nous ne sommes vraiment libres qu’en donnant un sens à ce que l’on vit selon ses buts. Il revient donc à l’individu de considérer si telle ou telle situation représente une entrave à sa liberté. La situation d’une personne est caractérisée par différents facteurs qui n’influent pas la liberté de l’individu.\nMa place&nbsp;: Le pays, l’appartement ou encore l’endroit où je suis présentement n’influe pas sur ma liberté parce qu’il ne dépend que de ma liberté de changer mon être-là, donc la place où je suis. Ainsi, c’est en comparant ma situation spatiale avec mes buts que je donnerais une signification à ma position.\nMon passé&nbsp;: Les actes que j’ai commis ne sont pas une contrainte à ma liberté puisqu’il ne revient qu’à moi de donner lui une signification selon mon projet de vie présent. En d’autres mots, je donne un sens à mon passé selon les choix que je fais présentement.\nMes entours&nbsp;: Les objets, l’en-soi, n’influent en rien mon existence, le pour-soi, et par le fait même ne représente pas une entrave à ma liberté.\nMon prochain&nbsp;: Les autres remplissent le monde dans lequel je vis de sens extérieur à moi que je n’ai pas choisi. Pourtant, il ne revient qu’à moi de choisir à les suivre ou non.\nMa mort&nbsp;: Ma mort représente un évènement absurde puisque je ne peux ni choisir ni interpréter le fait puisque je n’existerais plus. Sa mort n’est pas la limite ultime de ma liberté puisque ma conscience ne peut ni la concevoir ni l’atteindre ; la mort est néant. Elle est inévitable, mais elle ne m’empêche nullement de vivre ma vie selon mes buts avant son arrivée.\nChapitre 4 - Confrontation\nLes conceptions de Rousseau et de Sartre sont grandement différentes. Les différences sont liées aux différents enjeux des époques des auteurs. Effectivement, au temps de Rousseau, où le peuple était opprimé par la monarchie, la liberté était une lutte collective. En opposition à l’époque de Sartre, la liberté est plus ou moins chose acquise, relativement au régime monarchique, avec la démocratisation de l’Europe. Cette différence se fait sentir dans leur définition de la liberté. Pour Rousseau, la liberté est «&nbsp;dépendante&nbsp;» des relations entre les hommes dans la mesure où la société est une entrave à la liberté et un contrat social une solution à ce problème. La liberté est pour Rousseau collective puisqu’elle est sociale selon Rousseau, c’est un droit naturel qui doit être respecté par la société. Pour Sartre, la liberté est individuelle et fait part même de l’existence. Pour Sartre, l’homme est libre parce que, dépourvu d’essence, il est obligé de se choisir continuellement. Ainsi, la seule vraie entrave à sa liberté est soi-même, avec la mauvaise foi, puisque la liberté sartrienne est absolue, quoiqu’en situation. L’homme se construit lui-même ; sa liberté est par rapport son existence personnelle.\nAutre différence, pour Rousseau la société peut empêcher l’homme à exercer sa liberté, pourtant chez Sartre, la contrainte est en quelque sorte psychologique dans la mauvaise foi. Les deux auteurs font contraste quant à la nature des entraves à la liberté. Rousseau reflète la situation de son époque où les pauvres étaient oppressés par les riches, la contrainte est extérieure. Tandis que pour Sartre, puisque la liberté consiste à se choisir (se construire), la seule entrave est la mauvaise foi qui consiste à se faire croire et faire croire aux autres que nous n’avons pas le choix. Pour Sartre, la liberté est avant tout dans les choix ; elle est donc personnelle. Sartre et Rousseau s’opposent sur la personnalité ou la collectivité de la liberté puisque pour Rousseau, c’est les rapports sociaux qu’il entretient qui définissent sa liberté tandis que pour Sartre, l’homme est toujours libre puisqu’il est obligé de se construire avec chaque acte, peu importe la situation.\nPour Rousseau, en société, on n’acquiert sa liberté qu’en renonçant à ses intérêts personnels pour participer au contrat social. Pour Sartre, c’est plutôt en se choisissant que nous sommes libres. Leurs opinions diffèrent pour ce qui est de la collectivité ou de l’individualité de la liberté. Rousseau conçoit la liberté par le contrat social comme un accord entre plusieurs individus pour devenir citoyen&nbsp;: ils se plient aux lois en échange de leur liberté. Pour Sartre, il s’agit de se choisir, donc de se construire plutôt que de s’effacer comme dans le contrat social. Cette différence peut aussi s’expliquer par le contexte historique des auteurs et aux évènements de leurs temps.\nChapitre 5 - Prise de position\nLa conception naturaliste de Rousseau est intéressante parce qu’elle fournit des explications d’ordre naturel à la question de la liberté de l’homme. L’état de nature, malgré son état hypothétique, offre une bonne puissance explicative quant à la soif de liberté des hommes&nbsp;: la liberté est un don, un droit, pour l’homme. Dans le contexte historique du siècle des Lumières, ce changement de perception de l’homme alimenta de nombreuses révolutions en Europe. La pensée de Rousseau offre un contraste étonnant avec celle d’Aristote qui juge que certains hommes sont nés pour être esclave. L’égalité et la liberté sont beaucoup plus profitables à l’homme, qui peut s’épanouir, que l’esclavagisme ou la sujétion. Je suis d’accord avec Rousseau sur ce point, l’homme par nature est libre et on doit respecter ce droit.\nEncore selon Rousseau, la société aurait un effet négatif sur l’homme&nbsp;: les puissants abusent des pauvres en les privant de leurs libertés. Encore aujourd’hui, il est évident que ceux qui ont le pouvoir en abusent allègrement comme en Corée du Nord. Dans de telles conditions, il est flagrant que la société peut brimer la liberté des individus. Pourtant, dans des sociétés démocratiques comme la nôtre, cette affirmation semble moins vraie puisqu’il y a en quelque sorte un «&nbsp;contrat social&nbsp;». Il y a des lois, et nous nous devons de les suivre. En échange, nous sommes égaux face à celles-ci et nous profitons de davantage de liberté. Par contre, s’il y a une certaine corrélation avec le contrat social, les intérêts individuels ne se sont pas estompés pour autant. C’est pourquoi la conception naturaliste de Rousseau semble moins s’appliquer à notre époque post-moderne qu’au siècle des Lumières. La liberté de notre époque est plus individuelle comparativement à la liberté qui est une lutte collective pour Rousseau.\nSi je suis d’accord sur certains points avec Rousseau, je trouve que sa pensée s’applique mal à notre époque post-moderne puisque nous tenons pour acquis ce qui était jadis une lutte de tous les peuples.\nSartre\nLa conception existentialiste de Sartre est très reliée à l’humanisme ; l’homme est le seul responsable de ses actes et il se choisit continuellement. Malgré le côté un peu métaphysique de l’essence des choses, la confrontation en-soi/pour-soi justifie la versatilité de l’homme ; contrairement à un objet, l’homme peut occuper plusieurs fonctions selon sa volonté. Un homme n’est pas avant sa naissance désigné à occuper telle ou telle fonction. C’est un concept très fort et optimiste pour l’homme, sa liberté est absolue, mais en situation. Je suis d’accord avec le concept de liberté en situation, effectivement, nous ne pouvons choisir ou nous choisir (se construire) que devant une situation, puisque chaque acte possède un contexte réel. Nous ne pouvons pas exercer notre liberté hors contexte. Étant le seul responsable de l’orientation de son projet, l’angoisse face à cette responsabilité est d’autant plus lourde. Je ne peux mettre la faute sur autrui, si je fais quelque chose, c’est totalement de ma responsabilité. Je suis totalement d’accord avec le concept de la mauvaise foi&nbsp;: devant tant de responsabilités, il est facile de se dire que nous n’avons pas le choix. Aussi, sa conception est plus représentative de notre époque, les enjeux du temps de Sartre sont plus ou moins encore les mêmes qu’à notre époque post-moderne. La recherche de soi-même, l’insécurité et la responsabilité de ses actes sont encore au cœur de notre culture. Sa pensée cadre parfaitement avec des concepts que tous vivent quotidiennement.\nRousseau contre Sartre\nD’entre les deux conceptions de Rousseau et de Sartre, je suis davantage en accord avec celle de Sartre. Son époque est très semblable à la nôtre, permettant à sa conception de définir l’homme moderne et post-moderne dans sa réalité contemporaine avec plus de fidélité que la conception de Rousseau qui s’applique plus à un monde prérévolution française. Il y a aussi toute la question de «&nbsp;s’effacer&nbsp;» pour suivre l’intérêt collectif qui me dérange, l’individualité est une grande force de l’être humain et l’on ne doit pas perdre ses propres intérêts. C’est pour cela que le «&nbsp;se choisir&nbsp;» de Sartre cadre plus dans mes valeurs individualisme.\nConclusion\nLa liberté a toujours été importante aux hommes, autant dans le siècle de lutte que fut le siècle des Lumières qu’après les deux guerres mondiales. Peu importe la forme, elle reste un enjeu perpétuellement d’actualité que ce soit avec les révolutions des peuples opprimés par des dictateurs du printemps arable de 2010 ou des luttes pour la liberté d’expression dans plusieurs pays.\nReferences\nD’autres textes ont été consultés pour la rédaction de cet essai: 1, 2, 3, 4, 5, 6, 7.\nROUSSEAU, Jean-Jacques. Du contrat social. Ed Flammarion, 2012, 225 p. ↩ ↩2\nSARTRE, Jean-Paul. L’existentialisme est un humanisme. Ed Gallimard, 1996, 111 p. ↩ ↩2\nCUERRIER, Jacques. L’être humain. Ed Chenelière éducation, 2009, 268 p. ↩\nROWELL, Vincent. Philosophie, Les conceptions de l’être humain. Ed Études Vivantes, 1998, 306 p. ↩\nST-ONGE, J.-Claude. La condition humaine. Ed Chenelière éducation, 2011, 232 p. ↩\nMANON, Simone (professeure de philosophie). PhiloLog http://www.philolog.fr/, 23/04/16. ↩\nROUSSEAU, Jean-Jacques. Discours sur l’origine et les fondements de l’inégalité parmi les hommes. Éd. Cégep de Chicoutimi. 2016, 124 p. ↩"},{"url":"https://willguimont.com/blog/bash-docker/","title":"Atelier Bash et Docker","body":"Ces ateliers ont été conçus pour le département d’informatique et de génie logiciel de l’Université Laval.\nIls ont été donnés dans le cadre du cours IFT-2001 et GLO-2001 Systèmes d’exploitation.\nAtelier d’introduction à Bash\nCe premier atelier a pour but de vous familiariser avec le terminal et le langage de script Bash.\nVous apprendrez à utiliser les commandes de base de Bash et à écrire des scripts simples.\nÉnoncé Bash\nSolutionnaire Bash\nCode source\nAtelier d’introduction à Docker\nCe deuxième atelier a pour but de vous familiariser avec Docker.\nVous apprendrez à utiliser les commandes de base de Docker et à écrire des Dockerfile.\nÉnoncé Docker\nSolutionnaire Docker\nCode source"},{"url":"https://willguimont.com/blog/maizerets-2/","title":"Maizerets 2","body":"Here are some pictures I took in Domaine de Maizerets in Quebec City."},{"url":"https://willguimont.com/blog/maskbev/","title":"MaskBEV","body":"Accepted paper at IROS 2023: MaskBEV: Joint Object Detection and Footprint Completion for Bird’s-eye View 3D Point Clouds\nRecent works in object detection in LiDAR point clouds mostly focus on predicting bounding boxes around objects. This prediction is commonly achieved using anchor-based or anchor-free detectors that predict bounding boxes, requiring significant explicit prior knowledge about the objects to work properly. To remedy these limitations, we propose MaskBEV, a bird’s-eye view (BEV) mask-based object detector neural architecture. MaskBEV predicts a set of BEV instance masks that represent the footprints of detected objects. Moreover, our approach allows object detection and footprint completion in a single pass. MaskBEV also reformulates the detection problem purely in terms of classification, doing away with regression usually done to predict bounding boxes. We evaluate the performance of MaskBEV on both SemanticKITTI and KITTI datasets while analyzing the architecture advantages and limitations."},{"url":"https://willguimont.com/blog/obsidian-android/","title":"Synchronise Obsidian vaults to Android using Git","body":"Here’s the procedure to sync your Obsidian vault to an Android phone.\nFirst, you’ll have to run these commands on your computer:\nCreate a SSH key using ssh-keygen -f ~/obsidian-phone-key -t ed25519 -C \"your_email@example.com\"\nAdd the public key (~/obsidian-phone-key.pub) to your GitHub account\nConnect your phone to your computer and allow the computer to access phone data\nCopy the private key (~/obsidian-phone-key) to your phone\nAlso copy your Obsidian vault, including the .git folder\nNow, on your phone:\nInstall F-Droid\nOpen F-Droid and install MGit\nFrom the settings page of MGit, import the SSH key\nSSH Keys\nDownload button in the upper right corner\nSelect obsidian-phone-key you copied from your computer\nGo back to the main menu of MGit, press the three dots in the upper right and press the Import Repository button\nBrowse to the repository you copied from your computer\nYou should now be able to pull from MGit\nYou can now open Obsidian\nOpen folder as vault\nBrowse to the repository you copied from your computer\nYour Obsidian vault is now synced using Git!"},{"url":"https://willguimont.com/blog/transformers/","title":"Learn Transformers","body":"Here’s a small library I made to explain the Transformer architecture in PyTorch: willGuimont/transformers.\nThe code is heavily commented and should be easy to follow.\nThis repository also contains some popular transformer-based models, such as Vision Transformer.\nThis was made while developping new practical exercises for GLO-7030 at Université Laval."},{"url":"https://willguimont.com/blog/torch-waymo/","title":"torch_waymo","body":"Here’s a small library I made to use the Waymo Open Dataset in PyTorch, without any dependency to Tensorflow: willGuimont/torch_waymo.\nHere’s the corresponding PyPI package: torch-waymo."},{"url":"https://willguimont.com/blog/maizerets/","title":"Maizerets","body":"Here are some pictures I took in Domaine de Maizerets in Quebec City."},{"url":"https://willguimont.com/blog/nn-on-pc-ift-6001/","title":"Apprentissage profonds sur les nuages de points 3D","body":"Papier réalisé pour le cours IFT-6001 @ Université Laval sur l’apprentissage par réseaux de neurones profonds sur les nuages de points 3D: Apprentissage par réseaux de neurones profonds sur les nuages de points 3D"},{"url":"https://willguimont.com/blog/dashy-dango/","title":"Dashy Dango","body":"Dashy Dango a wave fighting game made with WASM-4 by @samX500 and @willGuimont.\nTry the game here or on itch.io!\nSource code is here: willGuimont/dashy-dango\nWe built our own ECS (entity-component-systems) from scratch in Rust.\nMore screenshots:"},{"url":"https://willguimont.com/blog/api-haskell/","title":"APIs using Haskell and polysemy","body":"Here’s a simple API in Haskell using Servant and Polysemy: willGuimont/status-checker.\nAnd another one: willGuimont/exercises_api"},{"url":"https://willguimont.com/blog/point-bert-ift-6001/","title":"Point-BERT IFT-6001","body":"Voici une présentation que j’ai donnée le 28 avril 2022 pour le cours IFT-6001 Introduction à la recherche en informatique : communication et méthodologie à l’Université Laval sur Point-BERT.\nVoici les diapositives: Point-BERT\nVoici l’enregistrement de la présentation:"},{"url":"https://willguimont.com/blog/tests-en-pratique/","title":"Les tests en pratique","body":"Tester, ce n’est pas seulement s’assurer que le code « marche ». C’est une démarche structurée pour réduire l’incertitude, détecter les erreurs tôt et donner confiance dans le système. Trop souvent, les tests sont vus comme une contrainte ou un ajout après coup. En pratique, bien utilisés, ils deviennent un outil de conception, de documentation et de robustesse.\nCes slides présentent une approche concrète : les différents niveaux de tests (unitaires, intégration, système, acceptation), leurs rôles respectifs et les pièges fréquents. On y retrouve aussi des principes simples : un test doit être automatisé, indépendant et binaire. Il doit rester lisible, rapide et maintenable.\nÀ travers l’exemple d’UTournament, nous verrons comment structurer un test (Arrange-Act-Assert), comment limiter la fragilité et comment élargir la portée avec des approches comme le property-based testing. L’objectif est de montrer qu’avec un minimum de rigueur, les tests deviennent un allié au développement plutôt qu’un fardeau.\nSlides: Les tests en pratique"},{"url":"https://willguimont.com/blog/compute-canada/","title":"Compute Canada Quickstart","body":"Here’s a small guide on how to use Compute Canada clusters to train deep learning models.\nIn this guide, we’ll do the necessary setup to get you up and running on a cluster. This guide is written to work with Narval, but it should be easy to adapt it to other clusters by changing the URLs. You can see available clusters at this URL.\nWe recommend Narval for deep learning as of now (2021-2022) because it has 4 NVIDIA A100 (40 Gb memory) per node.\nThis guide was written by William Guimont-Martin, 2021\nAlso available on Norlab Wiki.\nAccount creation\nFirst of all, you’ll need a Compute Canada account to access the clusters. To do so, please see Tips for new students. You’ll see the URL to apply for an account and Philippe Giguère’s sponsor code.\nYou’ll need your username and your password for the following steps.\nSSH and ssh-copy-id\nTo access Compute Canada’s clusters, you’ll need to ssh into them. You can get the login node URL by checking the cluster’s wiki page. For Narval, the URL is narval.computecanada.ca.\nTo connect to the cluster:\nYou’ll be asked to enter your password. If everything goes right, you’ll be greeted by the cluster’s welcome message.\nTo make things easier later, we recommend to set up an SSH key to connect to the cluster.\nTo do so, you’ll need to generate an SSH key if you don’t already have one you’d like to use.\nPlease see Generating a new SSH key and adding it to the ssh-agent from GitHub to generate a new SSH key.\nOnce you have added your SSH key and added it to the ssh-agent, copy your key to the cluster with\nIf everything went right, you should now be able to ssh into your cluster without being asked your password.\nClone your code\nNow that you are logged in the cluster, you should have a bash shell. From that shell, you can use most Linux commands you’re used to like ls, cd, etc..\nWe’ll now clone your project on the cluster.\nFrom the cluster, you can create another SSH key and add it to GitHub.\nThis will allow you to git clone your repository into your home directory.\nSetup your Python venv\nIt is recommended to create your Python venv from inside a SLURM job, if your installation script is quite complex, it will be easier to set up before starting any job.\nTo create a Python venv:\nOnce created, you can activate it using:\nYou should now see (venv) at the beginning of your shell prompt.\nYou can now pip install all of your dependencies.\nPlease note that when installing mmdetection and its dependencies, you might need to also run:\nIf you want to create your venv from inside a job, note the commands you did to setup your environment and copy them into your job script.\nPutting your data on the cluster\nTo put your data on the cluster, we recommend using sftp.\nFirst of all, take the time to familiarize yourself with the different types of storage available on the clusters.\nFrom that, you’ll have to upload your dataset in ~/projects/def-philg/&lt;username&gt;. Note that this space is shared across the people in the lab. Please keep it clean!\nFrom you local computer:\nIf you are using mmdetection, you might want to add a symlink to your data in code/data. To do so, run the following commands:\nSLURM jobs\nWe can now create jobs. To do so, create a .sh file and paste the following inside:\nThis script is organized in five main parts.\nConfigure job\nThis part is used to configure the resources needed for your job.\nSetup installation\nThis part sets environment variables for multithreading and loads needed modules. You might need to change the loaded versions.\nLoading the venv\nThis will load the venv we created earlier\nRunning your actual code\nYou will need to change this part to run what you want to run. This scripts shows how to use multi-GPU with mmdetection3d.\nCleaning up\nNot strictly needed, but it is always a good thing to clean up after yourself.\nRun the job\nTo run a job, you first have to queue it to the job scheduler. To do so, run the following command:\nYou can then verify the job is queued by running:\nMore information is available here\nUseful tools\nsbatch: queue a job.\nsq: view your queued jobs\nscancel &lt;id&gt;: cancel job with id\nsalloc --account=def-philg --gres=gpu:2 --cpus-per-task=4 --mem=32000M --time=5:00:00: start an interactive job, which will allow you to test your scripts before queuing jobs\nsftp: useful tool to transfer data from and to the cluster\ndiskusage_report: see used disk space"},{"url":"https://willguimont.com/blog/rolly-dango/","title":"Rolly Dango","body":"Rolly Dango, an isometric rolling puzzle made with WASM-4. We made it for the WASM-4 Game Jam.\nTry the game here or on itch.io!\nSource code is here: willGuimont/rolly-dango\nHere’s some key points of our project:\nWe made our own ECS (entity-component-systems) from scratch\nTo save cartridge space, we built our own Huffman coding algorithm\nTo help ourselves make levels, we built our own level editor using p5.js\nMade all of our sprites using Aseprite\nOur level editor:"},{"url":"https://willguimont.com/blog/boileau/","title":"L’Art poétique, Boileau","body":"J’apprécie beaucoup le poème L’Art poétique de Nicolas Boileau.\nDans l’extrait suivant, il aborde l’importance de la clarté de l’esprit lors de l’écriture: Il est impossible d’écrire clairement si l’on n’a pas une idée précise de ce que l’on veut dire. Depuis que j’ai découvert ce poème, j’y pense dès lors que je m’assois pour écrire.\nVoici un extrait que je trouve particulièrement intéressant:"},{"url":"https://willguimont.com/blog/monads/","title":"Understanding monads: Some resources","body":"Instead of doing like everyone else and write a long-winded blog post about my aha moment about monads, I’ll only link some useful resources about monads.\nCuddly, Octo-Palm Tree’s series on monads\nFunctors and Monads For People Who Have Read Too Many “Tutorials”"},{"url":"https://willguimont.com/blog/transformers-in-cv/","title":"Transformers in Computer Vision","body":"On September 2nd 2021, I gave a talk on transformer-based neural network architectures for computer vision. I gave the in University Laval’s Norlab.\nHere are the slides: Transformers in Computer Vision\nAnd here is the recording of the event:"},{"url":"https://willguimont.com/blog/ray-marching/","title":"Ray marching","body":"Ce post est un court résumé d’un article que j’ai écrit dans le cadre du cours IFT-3100 Infographie à l’Université Laval.\nJ’aimerais vous présentez une technique de rendu que je trouve très originale : le ray marching. Pour ce faire, je vous propose un résumé de cet article1 de Michael Walczyk.\nLe ray marching représente les objets dans la scène grâce à une fonction de distance signée. Cette fonction donne la distance entre un point et la surface d’un objet. Si le point est à l’extérieur de l’objet, alors la distance est positive, si le point est à l’intérieur alors la distance est négative et si le point est exactement sur la surface, sa distance sera nulle. Par exemple, la fonction de distance signée d’une sphère sera :\n$$\\lVert p - c \\rVert - r.$$\noù $p$ est le point, $c$ est le centre de la sphère et $r$ est le rayon de la sphère.\nOn a donc que pour chaque point dans l’espace, il est possible de connaître sa distance avec la surface.\nDe façon similaire au ray tracing, le ray marching procède par lancé de rayons. Toutefois, plutôt que de calculer directement l’intersection entre le rayon et les objets de la scène, ce qui pourrait se révéler ardu pour certaines formes, on utilise plutôt la fonction de distance signée.\nLe ray marching procède en se déplaçant progressivement du point de départ dans la direction dy rayon lancé. On va itérativement se déplacer pas à pas dans la scène tout en vérifiant si le rayon intersecte la surface. Si on rencontre une surface, alors on doit rendre ce pixel avec la couleur de l’objet. Une implémentation naïve pourrait procéder par de petits pas de taille fixe. Par contre, on a ici un tradeoff entre précision et temps de calcul. Une petite taille de pas va demander beaucoup d’itérations, donc beaucoup de temps de calcul, alors qu’une taille de pas trop grande va donner un rendu de mauvaise qualité.\nOn peut alors utiliser la fonction de distance signée pour nous indiquer la taille de pas à choisir pour chaque position intermédiaire. Puisque la fonction de distance signée donne la distance entre la position courante et la surface, on sait que l’on peut se déplacer de cette distance sans intersecter la surface. On peut alors obtenir un rendu de bonne qualité avec beaucoup moins de pas.\nL’image ci-dessous, tirée de l’article, illustre ce principe. Les points bleus représentent les positions de chaque itération de l’algorithme de ray marching. Les rayons des cercles représentent les tailles des pas. À chaque itération, on va se déplacer d’un pas de taille égale au rayon du cercle. Comme le cercle a pour rayon la distance avec la surface, on peut avancer de cette distance sans entrer en collision avec la surface.\nRay marching, figure extraite de 1\nOn peut ainsi réaliser quelques itérations par rayon, et si la fonction de distance signée devient plus petite qu’un seuil, donc assez près de zéro, alors on considère être rendu sur la surface, on rend alors le pixel. De façon similaire au ray tracing, on fait le rendu d’une image complète en « lançant / marchant » un rayon par pixel de l’image.\nJe trouve vraiment intéressante la représentation implicite des objets dans la scène. On peut rendre n’importe quel objet pour lequel on peut définir une fonction de distance signée. Cette représentation possède plusieurs particularités importantes, notamment lorsque l’on peut facilement combiner des objets. Suite à la lecture de cet article, j’ai poursuivi mes recherches sur le sujet du ray marching et j’ai trouvé différentes façons de combiner des objets dans une scène.\nOn peut faire une union d’objets en prenant la distance minimale entre le point et les différents objets. Pour combiner deux objets, on aurait $\\min(d[p, o_1], d[p, o_2])$ où $p$ est le point, $d[p, x]$ est la distance entre le point $p$ et l’objet $x$, $o_1$ et $o_2$ sont des objets dans la scène.\nDe façon similaire, on peut faire l’intersection de deux objets avec la distance maximale entre le point et les objets : $\\max(d[p, o_1], d[p, o_2])$.\nAvec l’intersection, il est possible de définir la différence entre deux formes, donc un volume auquel on vient enlever un autre volume. On utilise alors l’intersection entre le premier objet et le complément du second, que l’on obtient en inversant le signe de sa fonction de distance signée (l’intérieur devient l’extérieur et l’extérieur devient l’intérieur). La fonction de distance signée devient alors : $\\max(d[p, o_1], -d[p, o_2])$.\nIl est aussi possible de faire des combinaisons très intéressantes visuellement avec une fonction de type softmax. On obtient alors une union smooth entre des objets. La fonction est : $\\min(d[p, o_1], d[p, o_2]) - \\frac{h^3k}{6}$, où $k$ est le paramètre de smoothing et $h = \\frac{\\max(k - \\lvert d(p, o_1) - d(p, o_2) \\rvert, 0)}{k}$.\nPour explorer cette technique un peu plus en détail, j’ai implémenté un ray marcher dans openframeworks. Voici un rendu que j’ai réalisé avec une union smooth entre des sphères et un cube.\nLe code source est disponible ici : willGuimont/ray. À noter qu’il est possible de faire de l’illumination avec cette technique aussi. Il faut simplement approximer la normale en lançant des rayons autour du rayon de la caméra. Cette technique est vraiment très intéressante.\nJ’ai aussi implémenté le même algorithme en Rust: willGuimont/rust_ray\nRéférences\nM. Walczyk. Ray Marching. https://michaelwalczyk.com/blog-ray-marching.html ↩ ↩2"},{"url":"https://willguimont.com/blog/polymorphism/","title":"Inward and Outward Polymorphism","body":"I’ve recently become fond of Algebraic Data Types (ADT).\nLanguages like Haskell and Rust allows you to model your domain in a concise way using ADT. What would take several classes (and files) in Java (Interface-Oriented Programming (IOP)) can be expressed in a couple of lines. This got me thinking about software architecture using those two.\nBoth IOP and ADT have advantages and disadvantages.\nADT is a lot more concise but requires to use pattern-matching. Adding a new ADT type would then mean that you’ll have to add a case to every function matching on the type. Whereas adding a new function acting on an ADT is easy, you simply need to write the function.\nFor IOP on the other hand, adding a new implementation of the interface is easy. You simply implement the needed methods. Whereas adding a new function in the interface can quickly become expensive, you’ll have to add the method to every class implementing the interface. I’ll include Haskell’s type-classes and Rust’s traits as IOP.\nI discussed that compromise a bit more in a previous post on abstract data types. Albeit about a different type of ADT (algebraic vs abstract), the post is still relevant to understand the difference between ADT and IOP.\nHere, I would like to discuss the software engineering aspect of using IOP and ADT when designing modules. I’ll introduce the concept of inward and outward polymorphism.\nMotivation\nTo motivate the introduction of those concepts, let’s start with two small examples.\nAlgebraic Data Type\nYou are charged to build a networking library in Rust. You want to support both IPv4 and IPv6. Each one represents addresses in different ways, IPv4 stores address using four 8 bits numbers, while IPv6 stores it as a string. Using Rust, and having recently read the Rust Book’s chapter on enums, you decide to declare them the following way:\nThe user will call your functions, like connect_to(ip: IpAddr), using the polymorphic nature of IpAddr to use either IPv4 or IPv6.\nInterface-Oriented Programming\nThe next day, you are writing Java (poor you). You have to implement an application that provides support for plugins. So, you make an interface and write your code calling that interface. The user can then implement your interface with their code, and you can call their class since it implements your interface.\nSo, your library calls the user’s code polymorphically.\nInward or Outward\nIn the last two examples, we can see two different uses of polymorphism. In the first example, the user called your library passing polymorphic data type, while the second one, the user implemented an interface so that your library would call their code.\nWe can see that the flow of control is different in both cases. In the first example, the flow moves into your code whereas, in the other example, the flow moves out of your code.\nBased on the control flow, we can define inward polymorphism as polymorphism used when the user calls your code. Outward polymorphism would then be the use of polymorphism when your code is expected to call the user’s code.\nThe examples were about libraries, but the same concept of inward or outward can be applied to modules.\nADT or IOP\nInward and outward polymorphism impose different types of constraints.\nWhen I need to write inward polymorphic code, I like to use the expressiveness of ADT. This can make the code elegant and concise.\nFor outward polymorphism, you have to use IOP, so that both your module and the user’s code can communicate using a common interface.\nThe choice between ADT and IOP can be really difficult and, as always, there is no apply to all solutions in software architecture. The only real answer you’ll receive is “it depends”."},{"url":"https://willguimont.com/blog/frp/","title":"FRP: You don't own your android phone","body":"Recently, I had to change my phone. I got my grandfather’s old smartphone to replace it. I wanted to try a new OS to replace Google’s Android, I was thinking of trying either LineageOS or Ressurection REMIX OS. So, I candidly hard reset it by entering the “Recovery Menu”, just like I would on any computer I ever installed Linux on.\nExcept, there was something that went wrong. I got a weird error message. So, I booted the phone to see what was going on. I start the setup procedure. I am asked to log in with a Google Account, so I do. There, I get another weird message: I had to log in with the phone’s owner account…\nThe phone was locked. Locked by Google. The physical device that I thought I owned, locked by the inside by Google.\nThat particular “security” is called Factory Reset Protection. As far as I know, it is impossible to bypass that “protection”. Basically, once activated Google owns your phone.\nI lost a lot of time, had to ask my grandfather to enter a code on his new phone to unlock the one he gave me.\nWhen I was finally able to set up the phone, I was greeted by a lot of preinstalled and un-uninstallable applications. Truly, the smartphone oligopoly is a bad thing for users.\nBoth Android and iPhone phones are locked, you don’t own your phone like you own your computer.\nWe need more free and open-source alternatives, just like GNU/Linux is in the computer OS market."},{"url":"https://willguimont.com/blog/proof10101/","title":"Proof of sequence A094028","body":"1, 101, 10101, 1010101, 101010101, 10101010101, 1010101010101, 101010101010101, 10101010101010101, 1010101010101010101, 101010101010101010101, 10101010101010101010101, 1010101010101010101010101, 101010101010101010101010101, 10101010101010101010101010101\nThis is the sequence A094028 of the On-Line Encyclopedia of Integer Sequences (OEIS). According to a comment from Felix Fröhlich on the sequence’s page:\n101 is the only term that is prime, since (100^k-1)/99 = (10^k+1)/11 * (10^k-1)/9. When k is odd and not 1, (10^k+1)/11 is an integer &gt; 1 and thus (100^k-1)/99 is nonprime. When k is even and greater than 2, (100^k-1)/99 has the prime factor 101 and is nonprime.\nFelix Fröhlich, Oct 17 20151\nIt isn’t particularly striking that it should be the case. Let’s see why 101 is the only prime number of the sequence.\nThe sequence\nFirst of all, let’s study a bit more the sequence. Say $A(n)$ the $n$th element of the sequence (0-indexed).\nFor the first few elements we have:\n$$A(0)=1$$\n$$A(1)=101$$\n$$A(2)=10101$$\n$$A(3)=1010101$$\nWe can see that we can write the sequence as $$A(n)=\\sum_{k=0}^n 100^k.$$\nFor example, for the first element we have $$A(0)=100^0=1.$$\nSimilarly, for $n=2$, $$A(2)=10000 + 100 + 1 = 10101.$$\nA simpler form\nFor the $(n - 1)$th element of the sequence, we have:\n$$A(n) = 1 + 100 + 10000 + \\ldots + 100^{n-1} + 100^n.$$\nWe would like to remove some of terms, let’s consider\n$$100A(n) = 100 + 10000 + 1000000 + \\ldots + 100^{n} + 100^{n+1}.$$\nThis sequence has many similar terms to $A(n)$, let’s subtract $100A(n)$ from $A(n)$.\nWe can cancel the common terms, we end up with:\n$$A(n) - 100A(n) = 1 - 100^{n+1}.$$\nTo find out what $A(n)$ is, let’s isolate it from the equation.\n$$-99A(n)=1 - 100^{n+1}$$\n$$\\implies 99A(n)=100^{n+1} - 1$$\n$$\\implies A(n) = \\frac{100^{n+1} - 1}{99}.$$\nSo, we have: $A(n) = \\frac{100^{n+1} - 1}{99}$, which we can see from the comment in the OEIS’s website.\nCleaning things a bit\nWe can rewrite $A(n)$ as $A(n) = \\frac{(10^{n+1})^2 - 1}{99}$, because $100 = 10^2$.\nWe now have a difference of squares, we can factorize it further:\n$$A(n) = \\frac{(10^{n+1} + 1)(10^{n+1}-1)}{99}.$$\nTo make it a bit simpler to work with, let’s do a little variable substitution with $k=n-1$:\n$$A(k)=A(n-1)= \\frac{(10^{(n-1)+1} + 1)(10^{(n-1)+1}-1)}{99}=\\frac{(10^{n} + 1)(10^{n}-1)}{99}.$$\nTherefore,\n$$A(k)=\\frac{(10^{n} + 1)(10^{n}-1)}{99}.$$\nOkay, now we can write the $(k-1)$th element of the sequence a bit more easily, but it isn’t clear how we could prove that only 101 is prime. Let’s try to simplify this expression a bit more:\n$$A(k)=\\frac{(10^n + 1)(10^n-1)}{99}.$$\nFirst of all, we can see that $(10^n-1)$ will be of the form $\\underbrace{99\\ldots99}_{n\\text{-times}}$.\nWe can split the $99$ in the denominator into $9\\cdot 11$. We now have\n$$A(k)=\\frac{(10^n + 1)(\\overbrace{99\\ldots99}^{n\\text{-times}})}{9\\cdot 11}.$$\nWe can do the same for the $\\overbrace{99\\ldots99}^{n\\text{-times}}$ in the numerator, we get:\n$$A(k)=\\frac{(10^n + 1)(9\\cdot\\overbrace{11\\ldots11}^{n\\text{-times}})}{9\\cdot 11}.$$\nWe can cancel out the $9$s:\n$$A(k)=\\frac{(10^n + 1)(\\overbrace{11\\ldots11}^{n\\text{-times}})}{11}.$$\nDivision by 11\nLet’s take a small break from our sequence to discuss the divisibility by $11$. For a number $n$ to be divisible by $11$, we must have that $n \\equiv 0 \\pmod {11}$\nA number in decimal base can be expressed as $n=a_ka_{k-1}\\ldots a_1a_0$, so the value of $n$ is $10^ka_k+10^{k-1}a_{k-1}+\\ldots+10a_1+a_0$.\nNote that $10 \\equiv -1 \\pmod {11}$.\nSo,\n$$10^ka_k+10^{k-1}a_{k-1}+\\ldots+10a_1+a_0 \\equiv (-1)^ka_k+(-1)^{k-1}a_{k-1}+\\ldots+(-a_1)+a_0 \\pmod {11}.$$\nSince $(-1)^{2n} = 1 \\quad \\forall n \\in \\mathbf{N}$, we deduce that every even placed digit are added, whereas odd placed numbers are subtracted ($(-1)^{2n + 1} = -1 \\quad \\forall n \\in \\mathbf{N}$).\nSo, we can rewrite\n$$(-1)^ka_k+(-1)^{k-1}a_{k-1}+\\ldots+(-a_1)+a_0 \\equiv a_0 - a_1 + a_2 - a_3 + a_4 - a_5 \\ldots \\pmod {11}.$$\nSince we want to check if $n$ is divisible by $11$, we must check if\n$$a_0 - a_1 + a_2 - a_3 + a_4 - a_5 \\ldots \\equiv 0 \\pmod {11}.$$\nSo, $n$ is divisible by $11$ if the sum of even placed digit (starting from the rightmost position) minus the sum of the odd placed digit is divisible by $11$.\nThe proof\n$$A(k)=\\frac{(10^n + 1)(\\overbrace{11\\ldots11}^{n\\text{-times}})}{11}.$$\nNow that we know that when a number is divisible by $11$, let’s analyse $A(k)$.\n$n &gt; 1$ is even\nFirst, let’s suppose that $n$ is even. In that case, we get that $\\overbrace{11\\ldots11}^{n\\text{-times}}$ is divisible by $11$, because, since there is as much even placed $1$s as odd placed $1$s, the sum of even placed digit minus the sum of the odd placed digit is 0, which is divisible by $11$.\nNow, let’s use the variable $k=n-1$ to link the divisibility of $\\overbrace{11\\ldots11}^{n\\text{-times}}$ by $11$ with the sequence. We notice that for $n &gt; 1$ to be even, we must have that $k$ is odd, because $k=n-1$.\nFor $k\\geq3$, we have $n=k+1\\geq4$, so that $\\overbrace{11\\ldots11}^{(n\\geq4)\\text{-times}} &gt; 11$.\nThus, we can deduce that $\\frac{\\overbrace{11\\ldots11}^{(n\\geq4)\\text{-times}}}{11} &gt; 1$ and that it is an integer.\nPosing $a=\\frac{\\overbrace{11\\ldots11}^{(n\\geq4)\\text{-times}}}{11}$, we get $A(k) = (10^n + 1)a$.\nSince $A(k)$ is the result of the multiplication between two integers, each not equal to one, $A(k)$ is not prime.\n$n &gt; 1$ is odd\nWe can use a similar trick for any even $k\\geq2$, this time with the other part of $A(k)$. When $k$ is even (so that $n$ is odd) in $(10^n + 1)$, we have that one $1$ is at a even position and the other at an odd position, thus $1 - 1 = 0$ which is divisible by $11$. Similarly, we have that $(10^n + 1)&gt;11$ because $n\\geq3$. Thus, $\\frac{(10^n + 1)}{11}&gt;1$ and it is an integer. We can then conclude that $A(k)$ is not prime for even $k=n-1\\geq2$.\nThe remaining cases\nThis leaves us with two cases, $k=0$ and $k=1$. $A(0) = 1$ is not prime by definition and $A(1)$ is prime. Since for any other $k$, $A(k)$ is not prime, then $A(1)$ is the only prime in the sequence.\n$\\blacksquare$\nReferences\nOEIS. Sequence A094028, https://oeis.org/A094028 ↩"},{"url":"https://willguimont.com/blog/tossingbot/","title":"TossingBot","body":"This video presents TossingBot, a robotic arm that learns to throw arbitrary objects with residual physics.\nThis video was made for the class GLO-7030: Deep Learning at Université Laval."},{"url":"https://willguimont.com/blog/crocodiles-arctique/","title":"Des crocodiles et des tortues dans l’arctique canadien?","body":"Quand on pense aux îles de l’archipel arctique canadien, on ne s’imagine pas un décor rempli de palmiers, de crocodiles et de tortues.\nPourtant, c’était à cela que l’on pouvait s’attendre à l’époque de l’Éocène, qui s’étend d’il y a 56 à 33,9 millions d’années. Du pollen de palmiers fossilisés 1 et des fossiles de crocodiles et de tortues ont été retrouvés sur les îles d’Ellesmere et d’Axel Heiberg 2, deux îles du cercle polaire. Ces fossiles nous renseignent beaucoup sur le climat passé de ces îles. Effectivement, ces fossiles viennent d’animaux et de plantes peu résistants au froid ; le climat pendant l’Éocène devait être beaucoup plus chaud qu’aujourd’hui.\nIl n’y a qu’un seul problème, les scientifiques ne savent pas comment expliquer ce réchauffement. Des simulations de climat par ordinateurs ont montré que pour atteindre des températures pouvant accueillir ces animaux subtropicaux, il faut des concentrations en CO2 de près de 4000 parties par millions (ppm), ce qui contredit les quelque 2000&nbsp;ppm estimées selon des reconstructions de l’époque. Les simulations doivent alors faire des erreurs dans leurs approximations.\nLes nuages ont un impact important sur le climat et sont très difficiles à simuler. Les nuages participent à refroidir la Terre en reflétant une partie de l’énergie solaire vers l’espace. Les stratocumulus couvrent près de 20&nbsp;% des océans dans les régions subtropicales, donc s’il existe un mécanisme encore méconnu qui influence la présence de nuages, l’impact serait très important sur le climat. C’est justement ce que Schneider et al. ont investigué dans leur article scientifique Possible climate transitions from breakup of stratocumulus decks under greenhouse warming3.\nStratocumulus\nStratos: Bas de l’atmosphère\nCumulus: Forme arrondie\nStratocumulus: nuage bas dans l’atmosphère de forme arrondie\nPour bien comprendre pourquoi les simulations échouent à modéliser ce réchauffement de l’Arctique pendant l’Éocène, il faut savoir comme ces simulations fonctionnent. Pour simuler le climat, on doit résoudre plein d’équations qui décrivent la physique, les mouvements des fluides et la chimie de l’atmosphère. Or, ces équations sont dures à résoudre, surtout à l’échelle de la Terre. On doit alors faire des approximations. On découpe la Terre, son atmosphère et ses océans en une grille&nbsp;3D de l’ordre de la dizaine de kilomètres. On résout alors les équations pour chaque cube de la grille et on modélise les interactions entre les cubes. Ces simulations se nomment des modèles de climat globaux.\nCertains processus se déroulent à des échelles de distance ou de temps trop petites, comme les stratocumulus, pour être correctement modélisés. On a alors recours à des simplifications, appelées des paramétrisations. Ces paramétrisations sont reconnues comme étant peu précises. Alors si on veut modéliser correctement les stratocumulus, il faut utiliser des simulateurs plus fins.\nOn peut utiliser des simulations des grandes structures de la turbulence, ou Large Eddy Simulation (LES) en anglais. Ces simulations ont une résolution plus fine et permettent de simuler les nuages dans une région donnée.\nLa simulation globale et la LES s’influent l’une l’autre pour obtenir des simulations plus réalistes.\nMaintenant que l’on sait comment on peut simuler les stratocumulus, il faut comprendre leurs dynamiques. Comme on peut le voir dans la partie de gauche de la figure&nbsp;ci-dessous, le nuage est bordé par le haut par une masse d’air chaud et sec, et par le bas par de l’air humide venant de l’océan. Les gouttelettes d’eau du nuage sont très efficaces pour absorber et émettre les radiations à ondes longues, contrairement à l’air sec et chaud au-dessus du nuage. On a donc que les radiations à ondes longues viennent de régions plus hautes et plus froides de l’atmosphère. Le nuage reçoit moins d’énergie par rapport à ce qu’il émet, il refroidi donc. L’air froid du dessus du nuage est alors plus dense que le nuage. Étant plus dense, cet air commence à descendre dans le nuage, entraînant du même coup de l’air chaud et sec du dessus du nuage à l’intérieur. Cette convection permet aussi à l’air humide du dessous du nuage d’humidifier le nuage. Ces différents échanges permettent au nuage de rester présent.\nÉchanges de chaleurs dans les stratocumulus, figure extraite de 3\nSi on augmente la concentration en CO2, l’air au-dessus du nuage devient moins transparent aux radiations à ondes longues. Les radiations viennent alors de plus régions plus basses et chaudes de l’atmosphère. Les nuages reçoivent alors plus d’énergie. Recevant davantage de radiations, le nuage refroidit moins, ce qui limite la convection qui permet au nuage de s’approvisionner en humidité, ce qui rend le nuage instable.\nAussi, comme la température de l’eau est plus élevée, il y a davantage d’évaporation. Lorsque l’eau se condense dans l’air, elle libère de la chaleur, ce qui chauffe le nuage par en dessous. Ceci augmente la force des turbulences qui apportent de l’air chaud et sec dans le nuage, désintégrant le nuage du même coup.\nSi la concentration en CO2 augmente, il y aura alors moins de stratocumulus.\nLes simulations commencent avec une concentration de CO2 semblable aux niveaux actuels, soit environ 400&nbsp;ppm. Les chercheurs ont alors augmenté la concentration de CO2. Au fur et à mesure que la concentration augmente, on note que l’humidité dans les nuages diminue jusqu’à ce qu’ils se désintègrent à des concentrations supérieures à 1200&nbsp;ppm. Dès lors, comme il y a moins de nuages, moins d’énergie est reflétée vers l’espace. La température de l’eau à la surface aux tropiques augmente de 8°C et de 10°C dans les régions subtropicales.\nDonc, la disparition des nuages avec l’augmentation de CO2 dans l’atmosphère pourrait expliquer pourquoi l’on retrouvait autrefois des palmiers et des crocodiles dans l’Arctique canadien.\nSelon les estimations des chercheurs, on pourrait atteindre ce point si l’on continue de produire des gaz à effet de serre à notre rythme présent en une centaine d’années. On note aussi que le retour en arrière n’est pas aussi facile que de diminuer la concentration sous 1200&nbsp;ppm. Selon leurs simulations, les nuages ne reviennent que lorsque la concentration tombe sous les 300&nbsp;ppm.\nRéférences\nSource principale: Schneider3.\nLecture intéressante: Silberg4.\nA. Sluijs, S. Schouten, T. H. Donders et al. 2009. Warm and wet conditions in the Arctic region during Eocene Thermal Maximum 2. Nature Geoscience. 2019 ↩\nJ. Everle, M. D. Gottfried, J. H. Hutchison, C. A. Brochu, 2014. First Record of Eocene Bony Fishes and Crocodyliforms from Canada’s Western Arctic. Public Library of Science. 2014. ↩\nT. Schneider, C. M. Kaul, K. G. Pressel, 2019. Possible climate transition from breakup of stratocumulus decks under greenhouse warming. Nature Geoscience. 2019. ↩ ↩2 ↩3\nSilberg, Bob. 2020. Clouds, Arctic Crocodiles and a New Climate Model. NASA&nbsp;: Globa Climate Change. https://climate.nasa.gov/news/2936/clouds-arctic-crocodiles-and-a-new-climate-model/. ↩"},{"url":"https://willguimont.com/blog/on-learning/","title":"On Learning","body":"This is just some rambles on learning…\nI’m currently taking a class on deep neural networks. In the first class, we discussed multiple ways to have artificial intelligence systems.\nOne way would be to code hand-designed programs to accomplish the task. If something happens, then the system does that, else if this other thing happens, then do this, else if… We can achieve great things using rule-based programs, but on more complex tasks we would need a lot of cases to handle a lot of different cases.\nAnother way would be to make the computer learn from the data. So we analyze the data, find features for the system to learn from, then use classical machine learning algorithms such as SVM. Again, for complex tasks, we may miss some useful feature to correctly learn from the data.\nThe next step is to make the algorithm find features by itself. This is what neural networks do. Each layer of a deep neural network extract features for the higher layers to “reason” about. The classical example is a neural network that detects faces. The first layers might detect very simple patterns like edges. Higher up, a layer might detect corners and contours. Layers at the end of the network might detect complex object parts used by the final layer to classify the object. This is what is called representation learning, the artificial intelligence system finds a way of representing the data so that it is easy to solve the task.1.\nA good way of visualizing the different takes on artificial intelligence is from the book Deep Learning1.\nDreyfus\nWhen I learned about representation learning, it got me thinking about how we learn and the Dreyfus model2.\nI think we can draw links between the Novice stage and rule-based systems. In this first stage of the Dreyfus model, the learner relies on rules to determine the action.\nPerhaps, there are other links between machine learning and the Dreyfus model.\nMental representations\nIn The Sciences of the artificial3 from Herbert A. Simon, the author discusses the experience of A. de Groot and al. on chess perception. They showed chessboard with pieces in positions of real games for 5 seconds, then removed the pieces. Then they asked the subjects to put the pieces back in their position.\nAs you might guess, somebody not accustomed to chess would have a pretty hard time to reconstruct the positions. For grandmasters and masters though, they were able to reconstruct the positions with almost no error!\nThey then retried the experiment with random piece positions. In that case, even grandmasters could not reconstruct the board.\nThis shows that a grandmaster or a master built a good mental representation of the game. Instead of memorizing all pieces, they memorized relations between pieces, which allow them to memorize them more easily.\nSo, when I learned about representation learning, this reminded me of that particular example. Under the hood, a neural network tries to do exactly what the grandmaster did, build a good representation of the problem to solve. Once the representation is built, a simple linear layer can solve the problem!\nI think this is what is really going on when humans learn. All the studying isn’t about memorizing facts or formulas, it’s about building a good mental representation that can be used to solve many problems. Once you understand basic algebra, you can use that framework to solve many problems you weren’t trained on.\nIn a neural network, we are usually not concerned about memorizing all the input data, instead, we want to generalize to unseen examples.\nPerhaps we should tell students that they aren’t in school to memorize facts, but instead to build a toolbox of mental representations to help them in life. Our brain is a neural network that needs to be trained with great care!\nSources\nGoodfellow, Ian &amp; Bengio, Yoshua &amp; Courville, Aaron. Deep Learning. 2016. ↩ ↩2\nDreyfus, Stuart E. The Five-Stage Model of Adult Skill Acquisition. 2004. ↩\nSimon, Herbert A. The Sciences of the Artificial, 1969. ↩"},{"url":"https://willguimont.com/blog/prosac-algorithm/","title":"PROSAC","body":"Recently, for a graduated mobile robotics class, I had to present a scientific paper. Loving algorithms, I decided to read Matching with PROSAC - Progressive Sample Concensus1, a paper presenting a variant of the popular RANSAC algorithm.\nThis article will be a more detailed version of the presentation I gave for that class, the slides are here. I won’t go into much of the mathematics, the details are in the paper. What I want to do with this article is to give you the intuition behind the algorithm more than writing a lot of equations.\nTo supplement the presentation, I wrote an implementation of PROSAC in Python: willGuimont/PROSAC.\nBefore jumping in PROSAC, let’s just review RANSAC a bit.\nRANSAC - Random sample consensus\nThe main goal of RANSAC is to estimate a model from noised data with outliers.\nThe basic idea is to randomly sample points from all points; fit a model on those randomly chosen points; then check if the model fits with the rest of the data.\nAs you can see, the algorithm is relatively simple: sample, fit, check, rinse and repeat.\nPseudocode\nLet $m$ be the minimum required number of points to fit a model. For a linear model in 2D, $m$ would be equal to $2$.\nLet $M$ a model to fit on the data.\nLet $\\epsilon_{tol}$ be the maximum error between a point and the model to be considered an inlier.\nLet $\\tau$ the fraction of inlier over the total number of points above which we are satisfied by the model. If we achieve that ratio, we stop early.\nLet $w$ be the probability of a given point to be an inlier. This is used to estimate the number of times we have to run RANSAC before finding a satisfying solution.\nLet $I$ be the set of inliers of maximal cardinality.\nRANSAC\nSample $m$ points\nEstimate a model $M$ from the $m$ sampled points\nFind the number of inliers with model $M$ with tolerance $\\epsilon_{tol}$. If the number of inliers is greater than the number of previously found inliers, replace $I$ with the new set of inliers.\nIf the fraction of inliers over the total number of points is greater than $\\tau$, estimate the model with all inliers and stop.\nRepeat steps 1 to 4, a maximum of $\\frac{1}{w^m}$ times\nAfter $\\frac{1}{w^m}$ iterations, estimate the model with $I$.\nLimitations of RANSAC\nIn the paper, they looked at estimating epipolar geometry from a pair of pictures.\nWith this pair of pictures, there is a lot of repetitive patterns on the floor and leaves, so there will be a lot of false matches. We have $m=7$ (epipolar geometry) and $w=9.2%$. The number of iteration of RANSAC is given by $\\frac{1}{w^m}$. The estimated number of samples is over $8.43 \\times 10^7$ !!! RANSAC would take a pretty long time before finding a good solution… That’s not realistic!\nWhen the model is complex and the number of correct points is low, RANSAC often takes very long before finding a satisfying solution. Let’s see if PROSAC can help.\nPROSAC - Progressive sample consensus\nPROSAC adds the notion of quality to points with the basic assumption that points of greater quality have more chances to be correct.\nSo, to make the algorithm quicker, the algorithm starts by trying to fit with points of greater quality first. Then we progressively add points of lesser quality.\nIntuition\nLet’s say that the points in red are outliers. We would like to fit a model of complexity $m=2$ on those points.\nRANSAC\nPROSAC\n$p_1$\n$p_2$\n$p_2$\n$p_4$\n$p_3$\n$p_6$\n$p_4$\n$p_3$\n$p_5$\n$p_5$\n$p_6$\n$p_1$\nIn RANSAC, we would be sampling $2$ points among all points, so we would have a probability of $\\frac{3}{6}\\cdot\\frac{2}{5}=20%$. So, we would need, on average, $5$ samples before finding an uncontaminated sample.\nWhereas in PROSAC, we sort points by quality. The first points would be of higher quality. We would start sampling from the top 2 points, so we would sample $p_2$ and $p_4$, both of which are correct. In this case, we would have found an uncontaminated sample on the first draw.\nEven with this simple example, we can see the possible speedup.\nSimplified pseudo code\nPROSAC\nSort points by quality (highest quality first)\nConsider the first $m$ points ($n\\leftarrow m$)\nSample $m$ points from the top $n$\nFit the model\nVerify model with all points\nIf the stopping criteria are not met, repeat steps 3 to 6, adding progressively points ($n\\leftarrow n + 1$). Otherwise, fit the model on all inliers and terminate.\nQuality\nThere are multiple ways of defining quality. For feature matching on images, we could use the correlation of intensity around features on each image.\nIn the paper, they mention Lowe’s distance. Lowe’s distance is the ratio between the most and second most similar matches.\n$s_1$: distance of the most similar match\n$s_2$: distance of the second most similar match\n$\\text{distance} = \\frac{s_1}{s_2}$\nThis distance allows us to select the first points that are significantly better than any other. A match with a low Lowe’s distance has more chances of being correct.\nTo define quality, we could inverse the ratio.\n$\\text{quality} = \\frac{s_2}{s_1}$\nYou can use any metric you want, as long as the higher the quality is, the higher the probability of the point being correct is.\nFast sampling\nWe could simply sample $m$ points from the top $n$ points, but that would be inefficient. We could sample multiple times the same set of points. So, to better explore the new possibles sets of $m$ points, we use the fact that when adding a new point $p_{n+1}$, we add a number $a$ of new samples. Each new sample contains the new point $p_{n+1}$ and $m-1$ from the $n$ first points.\nSo, we can simply sample $a$ times, picking $p_{n+1}$ and $m-1$ points in the best $n$ points. That way, we ensure the quickly explore the possibles sets of $m$ points.\nEnd criterion\nThe algorithm has two stopping criteria.\nNon-random solution\nMaximality\nNon-random solution\nThis criterion states that for a solution to be considered correct, it must have more inliers than what would randomly happen for an incorrect model.\nThis is computed by estimating the probability that an incorrect model (fitted on a sample) is supported by a given point, not in the sample.\nThe solution is considered non-random if the probability of having its number of inliers by chance is smaller than a certain threshold (usually 5%).\nMaximality\nWe want to stop sampling when the chance of getting a model that fit more points is lower than a certain threshold. This is computed by looking at the odds of missing a set of inliers bigger than previously found after a number of draws. If this probability falls under a certain threshold (usually 5%), it is not worth continuing drawing and we terminate.\nCode\nNot having found any implementation of PROSAC online, I decided to post mine: willGuimont/PROSAC.\nSources\nChum, Ondrej &amp; Matas, Jiri. Matching with PROSAC - Progressive Sample Consensus. 2005. ↩"},{"url":"https://willguimont.com/blog/atom-feed/","title":"Atom and RSS Feed","body":"I just made an Atom Feed and an RSS Feed."},{"url":"https://willguimont.com/blog/moon-orbit-eccentricity/","title":"Calculate Moon's Orbit Eccentricity","body":"In my third session of CÉGEP, I decided to take a thermodynamic and astrophysics class. It was the kind of class that completely changed the way you see physics.\nI have always seen physics experiments as things you just cannot do at home. You always need some complicated apparatus. Yet, in this class, we calculated the eccentricity of the Moon’s orbit, a bit of rock spinning around Earth, using only a camera and a bit of imagination.\nThe idea is quite simple, as the Moon moves closer and further of the Earth, its visual size will vary. When the Moon is at its perigee (closest to the Earth), it will appear bigger than when the Moon is at its apogee (further from the Earth). If we can measure its visual size around its orbit, we could deduce the orbit’s eccentricity.\nSo, to get that data we took pictures of the Moon and measure its diameter in the pictures. If we use the same camera and don’t modify the focal length of the lens, the only thing that will make vary the Moon’s size in the picture will be the distance from Moon to Earth.\nHere is some pictures I took back then:\nIn each picture, we can measure the Moon’s diameter in pixel.\nWe can then compute the distance between Earth and the Moon and pixel in each picture using:\n$$d=\\frac{r}{tan \\left (\\frac{0.5°}{2} \\right )}$$\nWhere $r$ is the radius, in pixel, of the Moon in the picture. Notice that $0.5°$ is the angular size of the Moon from Earth. To understand this equation, try to draw a right triangle from the Earth to the Moon.\nUsing the date at which the photo was taken, we can compute its angular position relative to other photos. This can be done knowing that the Moon moves 360° around the Earth in $29.5$ days on average. Assuming the Moon’s speed does not change on its orbit, we can compute it’s angle using:\n$$\\theta_i = (D_i - D_1) \\cdot \\frac{360°}{29.5} \\bmod 360°$$\nWhere $D_i$ if the current picture, $D_1$ is the first picture (all positions will be relative to this one). Both dates are in days.\nWe can graph the Moon’s distance and its angular position.\nAnd tadaaaa! We get an ellipse.\nWe can then measure the smallest and largest distance between the ellipse and the origin (the Earth) to get respectively $a$, the apogee’s distance, and $p$, the perigee’s distance.\nUsing this simple equation, we can get the eccentricity of the Moon’s orbit:\n$$e=\\frac{a-p}{a+p}$$\nAnd there you go, you have computed the eccentricity of Moon’s orbit just using a camera.\nAs stated at the start, this experiment blew my mind when I took the course: we calculated the eccentricity of a big rock moving around the Earth just using a camera.\nWow."},{"url":"https://willguimont.com/blog/cube-timer/","title":"Rubik's Cube Timer in Vue.js","body":"Here a simple Rubik’s Cube Timer I made in Vue.js\nLink to the timer"},{"url":"https://willguimont.com/blog/physics-from-scratch/","title":"Physics from Scratch","body":"Physics classes were among my favorite back in secondary school and CEGEP. Now that I am studying software engineering at university, I don’t get to do a lot of physics. Yet, I still want to learn about the universe and how it works.\nFor some years, I wanted to reinvent physics. To use modern tools to rediscover physical laws. I could use computer vision to track moving objects, plot the motions in a Jupyter Notebook and deduce the laws of motions.\nTo summarize this project:\nWhat I cannot create, I do not understand\n– Richard Feynman"},{"url":"https://willguimont.com/blog/pomodoro-timer/","title":"Pomodoro Timer in Vue.js","body":"Here a simple Pomodoro Timer I made in Vue.js.\nLink to the timer"},{"url":"https://willguimont.com/blog/intro-photoresistance/","title":"Exercice conditions avec des photorésistances","body":"Le but de cet exercice est de vous faire pratiquer les conditions et la boucle principale. De plus, vous aurez la chance de vous pratiquer à lire de la documentation.\nPhotorésistance\nUne photorésistance est une composante électronique dont la résistance change selon la lumière qui l’éclaire. En bref, on peut regarder la tension aux bornes de la photorésistance.\nBut de l’exercice\nÉcrivez le code pour qu’une DEL s’allume lorsque la photorésistance est à l’ombre et éteindre la DEL lorsqu’elle est éclairée.\nLiens utiles\nanalogRead\nSerial pour afficher des informations à l’écran"},{"url":"https://willguimont.com/blog/intro-conditions-arduino/","title":"Introduction aux conditions et à la boucle principale","body":"Dans l’article précédant, nous avons vu comment faire clignoter une DEL 2 fois avant de s’éteindre. Maintenant, essayons de n’allumer la DEL que lorsqu’un bouton est enfoncé.\nEn mots, on voudrait demander à l’Arduino:\nSi le bouton est enfoncé\nAllumer la DEL\nAttendre 3 secondes\nÉteindre la DEL\nSinon\nLaisser la DEL éteinte\nPour ce faire, on branche un bouton de la façon suivante.\nLes zig-zags représentent la résistance. Le symbole avec les deux points vides et le bout de fil représente un bouton ou un interrupteur.\nFonctions de bases\nPin en lecture\nCette fonction permet de mettre la pin 2 en mode lecture.\nLecture d’une pin\nCette fonction retourne HIGH si la tension est près de 5V sur la pin, ou LOW si la tension est près de 0V.\nVariables\nOn aimerait pouvoir garder l’état du bouton pour pouvoir comparer sa valeur. Pour ce faire, on utilise des variables. Les variables sont comme des boîtes. On peut donner un nom à la boîte et mettre une valeur dedans.\nOn peut demander à l’Arduino de nous créer une boîte avec un certain nom. On peut ensuite mettre une valeur à l’intérieur. Si on veut, on peut mettre une autre valeur dans la boîte par la suite.\nExemple\nint correspond au type de la variable. maVariable est donc de type int. int veut dire integer, c’est-à-dire entier en français. Un nombre entier sont des nombres comme 0, 1, 2, 3, 4, -3, 42 et -245. On a donc que maVariable peut contenir des nombres entiers.\nOn commence par donner la valeur 4 à maVariable.\nEnsuite, on change cette valeur pour 2.\nEnsuite, on met dans maVariable la valeur de maVariable + 1. Comme maVariable valait 2, on a que maVariable est maintenant 2 + 1, donc 3.\nOn remarque que le signe = n’a pas le même sens qu’en mathématique. En mathématique, il serait faux de dire 2 = 3. Dans notre cas, on assigne la valeur de droite à la variable de gauche.\nRemarque: On aurait pu utiliser n’importe quel autre nom pour la variable.\nRemarque: D’autres opérateurs que l’addition (+) sont définis, on peut faire des soustractions (-), des multiplications (*), des division (/) et le modulo (%). La division est toutefois tronqué avec des int puisqu’un int ne peut contenir que des nombres entiers. Le modulo calcule le reste de la division. Pour le moment, ne vous inquiétez pas avec cela.\nStorer l’état du bouton dans une variable\nMaintenant que nous savons ce qu’est une variable et comment récupérer l’état du bouton, sauvegardons l’état du bouton dans une variable.\nVoilà, nous avons l’état du bouton dans etatBouton.\nConditions\nMaintenant que nous avons l’état du bouton dans une variable, on aimerait pouvoir réaliser certaines actions SI une condition est vraie et d’autres SINON.\nIl existe en C++ une façon de réaliser des branchements conditionnelles, c’est-à-dire de faire certaines chose si la condition est vraie. En voici un exemple simple:\nif veut dire si en français. Si on lit le code en français, on a:\nCréer une variable nommée x et mettre 5 dedans\nSi x est plus petit que 10 alors\nOn mets 10 dans x\nComme 5 est plus petit que 10, on a que la valeur de x est maintenant 10.\nVoici un second exemple:\nif veut dire si en français. Si on lit le code en français, on a:\nCréer une variable nommée x et mettre 42 dedans\nSi x est égal à 42 alors\nOn mets x - 10 dans x\nComme x est bien égal à 42, on soustrait 10 à x. x a donc la valeur de 32 après l’exécution.\nRemarque: == veut dire est égal à, à ne pas confondre avec = qui veut dire assigner\nDisons que nous voulons réaliser ceci:\nSoit un entier x qui vaut 39\nSi x == 42\nx = x - 10\nSinon\nx = 0\nComment pouvons nous traduire cela en code?\nIl existe le mot-clef else qui fait exactement cela.\nOn aurait alors\nComme x = 39, on a que la condition x == 42 est fausse, donc on n’exécute pas x = x - 10, on entre ensuite dans le else où on assigne 0 à x.\nMaintenant que nous savons comment faire des conditions, on peut allumer la DEL si l’état du bouton est à HIGH.\nIl y a toutefois un petit problème, la lumière ne s’allume ou ne s’éteint qu’au démarrage de l’Arduino. Pour changer l’état de la lumière, il faut appuyer sur le bouton, puis redémarrer l’Arduino avec le bouton reset. Ce n’est pas très pratique. On aimerait que cela se fasse en tout temps.\nPour palier à ce problème, on peut utiliser loop. Comme son nom l’indique, loop est une boucle, c’est-à-dire que le code dans cette fonction s’exécute continuellement après l’exécution de setup. Déplaçons donc ce qui doit être exécuté plusieurs fois dans loop.\nOn remarque que les pinMode sont restés dans setup. En effet, on ne veut réserver ces pin qu’une seule fois.\nAlors que le if et else a été déplacé dans loop, puisque l’on veut continuellement vérifier si le bouton est enfoncé.\nMaintenant, on a que la lumière s’allume et s’éteint dès que l’on appuie ou relâche le bouton."},{"url":"https://willguimont.com/blog/intro-arduino/","title":"Introduction à Arduino","body":"Qu’est-ce qu’Arduino\nThe UNO is the best board to get started with electronics and coding. If this is your first experience tinkering with the platform, the UNO is the most robust board you can start playing with. The UNO is the most used and documented board of the whole Arduino family.\n– Site officiel Arduino.cc\nIl s’agit d’une carte comprenant un micro-contrôleur. La puce sur la carte est un petit ordinateur que l’on peut programmer. Il y a un port USB pour envoyer le code sur la puce et communiquer avec un ordinateur. Les pins sur le côté permettent d’avoir des entrées-sorties vers le vrai monde. On peut activer des lumières et des moteurs. On peut aussi lire les données envoyées par un capteur de distance à l’ultrason ou une photorésistante pour savoir s’il fait noir.\nIntroduction à la programmation\nUn programme est une suite d’instructions exécutées les unes après les autres. On peut voir un programme comme une recette très précise pour réaliser des choses.\nOn pourrait vouloir donner la suite d’étape pour faire un sandwich au beurre d’arachides. On pourrait essayer de demander à l’ordinateur de faire:\nFaire un sandwich au beurre d’arachide\nToutefois, l’ordinateur ne comprend pas comment faire cela. On devrait alors décrire les étapes de façon plus précises de façon à ce que l’ordinateur comprenne. On aurait alors:\nOuvrir le sac contenant le pain\nSortir une tranche de pain\nSortir le pot de beurre d’arachide de l’armoire\nSortir un couteau\nOuvrir le pot\nTartiner le pain avec le couteau\nSortir une seconde tranche de pain\nPlacer la tranche sur l’autre qui est déjà beurrée\nOuf ! C’est beaucoup d’étapes pour un simple sandwich.\nPour programmer un robot, c’est très semblable. On ne peut pas dire au robot de:\nAvancer pendant 10 secondes\nIl faut plutôt lui dire:\nActionne les moteurs\nAttendre 10 secondes\nArrête les moteurs\nDEL et Arduino\nDEL: Diode Électro Luminescente\nEn bref, une DEL est une lumière qui ne demande que très peu d’énergie. On peut donc l’alimenter directement depuis l’Arduino.\nSi l’on applique une tension aux deux bornes de la DEL, elle s’allumera.\nUne DEL a deux “pattes”. L’une d’elles est légèrement plus longue, il s’agit de la patte positive. L’autre est la négative. Lors du branchement, il faut faire attention de bien brancher la DEL.\nFaire clignoter une DEL\nOn voudrait demander à l’Arduino de faire clignoter une la DEL deux fois en la laissant 2 secondes éteintes et 3 secondes allumées. En mots, on aurait:\nAllumer la DEL\nAttendre 3 secondes\nÉteindre la DEL\nAttendre 2 secondes\nAllumer la DEL\nAttendre 3 secondes\nÉteindre la DEL\nTélécharger Arduino\nIl est maintenant temps d’essayer de demander à l’Arduino de faire ces étapes. Téléchargeons le logiciel depuis le site officiel d’Arduino.\nOn peut télécharger le ZIP ne demandant pas d’installation à cette adresse.\nOn peut maintenant lancer l’application.\nPremiers pas en Arduino\nOn voit maintenant un code de base fournit par Arduino.\nsetup est exécuté qu’une fois au démarrage de l’Arduino.\nloop est ensuite exécuté continuellement.\nToutefois, avant de commencer à coder, branchons la DEL dans l’Arduino.\nDans le cadre de ce document, nous allons utiliser la pin numéro 13. Vous pouvez en choisir une autre si vous voulez. La longue patte doit être connectée dans la pin 13 et la courte au ground. Le ground est le 0 volt de référence de l’Arduino.\nPour empêcher de brûler la DEL, on place une résistance en série avec la DEL. La résistance doit être entre 220 et 280 Ohms. Elle peut être placée avant ou après la DEL.\nFonctions de base\nPour faire clignoter une DEL, on doit pouvoir être capable de l’allumer, de l’éteindre et d’attendre. Nous allons voir comment réaliser ces actions dans Arduino.\nConfiguration de la DEL\nOn doit indiquer à l’Arduino quelles pins on désire utiliser, dans notre cas il s’agit de la pin 13. On appelle alors la fonction suivante:\nOUTPUT signifie que l’on veut se servir de la pin comme d’une sortie, donc mettre une certaine tension à la pin plutôt que de lire la tension. On aurait pu utiliser INPUT pour initialiser la pin comme une entrée. Nous allons en voir une exemple par la suite.\nAllumer la DEL\nPour allumer la DEL, on doit mettre une certaine tension aux bornes de la DEL. Comme la patte négative est branchée au ground, on doit appliquer un voltage positif à la patte branchée dans la pin 13. Pour ce faire, on utilise la fonction suivante:\nHIGH signifie une tension positive. Dans le cas d’une Arduino UNO, on met 5 volts sur la pin 13.\nÉteindre la DEL\nPour l’éteindre, on procède de façon semblable.\nAttendre\nPour attendre un certain temps, on utilise la fonction:\nIci, 1000 représente le temps à attendre en millisecondes. On a qu’une seconde est 1000 ms. Par exemple, pour attendre 5 secondes, on devrait indiquer 5000 ms.\nMettre le tout ensemble\nReprenons la liste d’étapes à suivre pour faire clignoter notre DEL:\nAllumer la DEL\nAttendre 3 secondes\nÉteindre le DEL\nAttendre 2 secondes\nAllumer la DEL\nAttendre 3 secondes\nÉteindre la DEL\nOn peut maintenant “traduire” chaque étape en code. Il manque toutefois l’initialisation de la pin au début. Avec les informations un peu plus haut, on peut maintenant coder notre Arduino.\nOn peut maintenant connecter l’Arduino par USB à l’ordinateur.\nOn doit premièrement vérifier que l’Aduino est connectée dans Outils -&gt; Port. Il faut s’assurer que le port selectionné est bien celui de l’Arduino.\nOn peut envoyer le code sur l’Arduino avec le bouton “Téléverser” (la flèche vers la droite). Après un certain temps, l’Arduino devrait faire clignoter la DEL deux fois selon les délais définit.\nÀ l’infini\nEt si on désirait faire clignoter le DEL à l’infini? Il faudrait demander à l’Arduino de la faire clignoter pleins de fois. On pourrait avoir:\nEt on devrait continuer ainsi pendant longtemps… Heureusement, il y a une meilleure façon !\nLa fonction loop est exécutée continuellement après setup, il suffit donc d’avoir le code suivant:\nOn a donc que la lumière s’allume pour 3 secondes, et s’éteint pendant 3 secondes à l’infini. On remarque qu’il y a moins de code copier-collé !"},{"url":"https://willguimont.com/blog/stack/","title":"Stacks","body":"Stacks are very much like a stack of plates. When you don’t want to do the dishes, you simply push the dirty plate on top of the others. Whereas when you are washing them, you take to plate on the top of the stack and proceed to pop them all.\nIn the same way, the stack abstract data type allows you to push and pop elements on the top of the stack.1\nWe can see a stack as a bottle, you can only access the content from the top.\nSince the last element pushed on the stack is the first one to be popped, we say that the stack is a last in, first out (LIFO) data structure.\nWe can define our stack in an imperative manner. Each operation modifies a stack instance.\nWe could also define the stack in a functional manner. The operations applied to the stack is viewed as a mathematical function that maps the old state to the new state. Refer to the example below for more information.\nImperative Definitions\nAn imperative stack has two essential operations:\nOperationDescription\n$\\text{Push}(S, x)$Add the element $x$ on the stack $S$\n$\\text{Pop}(S)$Remove the most recently added element to $S$ and return it\nWe can add non-essential operations to ease programming. We note the non-essential operations:\nOperationDescription\n$\\text{Peek}(S)$Return the most recently added element in $S$ without removing it\n$\\text{Create}()$Create a new Stack\n$\\text{IsEmpty}(S)$Return a Boolean value specifying if the stack $S$ is empty\nAs $\\text{Peek}(S)$ can be defined in terms of $\\text{Push}$ and $\\text{Pop}$, we say that this operation is non-essential.\nIn pseudo code (Python), we could define $\\text{Peek}$ as:\nImperative Implementations\nStack ADT in C\nFor simplicity, the stack is implemented with an array, but it could be implemented using a linked list.\nNote that from the user point of view, stack_t is completely opaque to the user. Only stack.c knows the inner representation of stack_t, since we only declared stack_t in stack.h.\nWe could completely change the implementation detail and nothing would change from the user point of view.\nNotice that we pass the stack to the function so that they know on which instance work. This is akin to Python self.\nThe self argument is passed automatically to the object when calling the function on the instance. Both lines are equivalents.\nNote that the two statements are equivalent because we know that print_value is not overridden. That way we are sure that vp.print_value() is equivalent to ValuePrinter.print_value(vp).\nObject Oriented Stack in Java\nNote how easy it is to add new represents of the stack. This is one of the strong points of object-oriented programming compared to ADT.\nHere the stack is defined with an interface that differs from the way we defined stack_t in C. stack_t is an opaque type that only the operations know the inner details, whereas the interface declares the methods a stack has to implement.\nFor more information on the comparison of ADT and object, see Introduction to Abstract Data Types.\nFunctional Definitions\nOn the contrary to the imperative stack, a functional implementation doesn’t mutate the state of the stack. Instead, the operations are seen as mathematical functions that map from the old state to the new frame.\nMuch like a function $ f(x) = x^{2} $ that maps a value $x$ to its square. The initial value of $x$ stays unchanged.\nIn a functional paradigm, we need three operations to fully define a stack.\n$\\text{Push}(S, x)$: Returns a new stack with an element $x$ pushed on top of $S$\n$\\text{Pop}(S)$: Return the stack $S$ without its first element\n$\\text{Top}(S)$: Returns the value on top of $S$\nNote that we don’t need a $\\text{Create}$ operation because there is no instance. Instead, we define a special state $\\Lambda$ (or empty) that represents the empty state.\nAlso note that we don’t need an $\\text{isEmpty}$ operation either, because we can simply test the equality to $\\Lambda$.\nFunctional Implementations in Haskell\nThis bit of code is adapted from haskell.org2.\nNote here that this code is much shorter and that the stacks aren’t mutated, instead, the functions return a new stack each time.\nSummary\nStacks are an elementary data structure. They are fundamentals in computer science. We note their uses in multiples places such as solving sudokus using backtracking algorithms, recursion, converting equations to reverse Polish notation and much more. We even see stacks used in how CPUs handle subroutines. They truly are fundamental to the computer science field, from the theoretical to the practical.\nReferences\nThomas H. Cormen et al. Introduction to Algorithms. 2009. ↩\nHaskell org. Abstract Data Types. 2014. ↩"},{"url":"https://willguimont.com/blog/ducks/","title":"Ducks","body":"Here are some pictures of ducks I took in the last few years. Most of them are from the Domaine de Maizerets in Quebec City."},{"url":"https://willguimont.com/blog/squirrels/","title":"Squirrels","body":"Here are some pictures of squirrels I took in the last few years. Most of them are from the Domaine de Maizerets in Quebec City."},{"url":"https://willguimont.com/blog/abstract-data-type/","title":"Introduction to Abstract Data Types","body":"The proposition of abstract data types by Liskov and Zilles is an important step in the way we build computer software. Abstract data types allow programmers to leverage the power of abstraction. By hiding implementation details, they allow programmers to use a structure by only being concerned about their behaviors and not how the structure works inside.\nThis gives programmers tools to think at a higher level, focused on the problem to solve.\nHistory\nAbstract data types (ADT) were proposed in 1974 by, 2008 Turing Award winner, Barbara Liskov and her coauthor Stephen Zilles in the paper Programming with abstract data types1. This paper wasn’t only a theoretical paper, they implemented the concept in a programming language. Liskov and her students created the programming language CLU that included abstract data types. ADTs are often seen as a step in the development of object-oriented programming.\nADTs were proposed in the 70s, around the time when the term software crisis was coined. People wanted to build sophisticated software programs but didn’t know how to build them effectively. Projects often ran over-budget, over-time, and the resulting software was often unmanageable, hard to maintain, and of low quality.\nIt was around that time that the use of the Go To statement was still controversial. Dijkstra wrote Go To Statement Considered Harmful2 in 1968. At that time, the use of Go To statements was widespread, which led to hard to debug and maintain codes.\nThe only kind of abstraction commonly used was procedures that weren’t necessarily used for abstraction per see but save space. If a bit of code was called often, they’d factor it out in a procedure, but not necessarily in a logical module. 3\nMoreover, languages like Algol60 used global variables for the communication between modules, which led to spaghetti code when scaling up. 3\nThe landscape of software programming was a bit messy, people were building software programs in a non-structured manner.\nThe ideas discussed in Liskov &amp; Zilles’s paper may seem obvious now but remember the time when they were proposed. In the last 50 years of building software programs, we learned a lot. This concept paved the way for object-oriented programming and better software design.\nDefinitions\nADT\nLiskov &amp; Zilles describe ADT as following:\nAn abstract data type defines a class of abstract objects which is completely characterized by the operations available on those objects. This means that an abstract data type can be defined by defining the characterizing operations for that type. […] Implementation information, such as how the object is represented in storage, is only needed when defining how the characterizing operations are to be implemented. The user of the object is not required to know or supply this information.\n– Liskov &amp; Zilles1\nWe learn from the definition that an ADT defines a type and a set of operations. From the perspective of the user, the type is completely opaque, meaning we can’t see the inner representation of the data it contains.\nThe type is completely opaque from outside of its operations. This contributes to build abstraction. For the user, the type is nothing particular, he has to use the operations.\nIf we want to have multiple representations for an ADT, the operations need to be aware of them. Adding a new representation in an ADT is therefore costly, we need to modify the existing operations. We will discuss later the implications of that fact.\nWe can observe two categories of operations: essentials and non-essential. Basically, non-essential can be defined from essential operations. We will see an example later in this article.\nAbstraction\nLiskov &amp; Zilles describe abstraction:\nWhat we desire from an abstraction is a mechanism which permits the expression of relevant details and the suppression of irrelevant details.\n– Liskov &amp; Zilles1\nThis allows programmers to use complex objects by being only concerned by their behaviors and not how they are implemented. This helps to have a higher level of reasoning, we can solve the problem using abstraction, then solve the subproblems that are those abstractions. This allows splitting the source code in logical modules, thus improving code reusability. We also separate the level of abstraction, the high-level code isn’t polluted by low-level details, therefore improving code readability.\nAs Robert C. Martin said in Clean Code:\nMixing levels of abstraction within a function is always confusing. Readers may not be able to tell whether a particular expression is an essential concept or a detail.\n– Robert C. Martin4\nA small example\nTo make it concrete, here a simple example.\nAbstract data types are like the built-in types of a programming language. When a programmer uses the type Integer of a programming language, he’s usually not concerned about the binary representation of the integer, i.e. a string of bits. He thinks about the Integer and the operations related to it as indivisible and atomic, when in fact they can be translated to multiple machine instructions. We define the operations on Integer by their behaviors. We expect (+) to add two Integer and (*) to multiply them, but we’re usually not concerned about how those operations are made under the hood. Abstract data types bring that idea of abstraction to user-defined structures. The client code can use those structures “as-is”, much like Integers, without knowing the implementation details.\nHowever, the abstraction is not perfect for most types. In the case of Integer, we may be interested in knowing if an overflow happened. The overflow depends on the implementation. A 32 bits Integer will overflow at a lower value than 64 bits one. More generally, most built-in types are not perfect abstraction because, in practice, we need to know when the implementation fails to represent the data.\nComputational complexity and ADT\nIn a more practical context, we consider that the behavior of the object isn’t enough to characterize completely the type. Let’s imagine we have two implementations of the Stack ADT (see example below). In the first implementation, the push operation is done in $$O(1)$$. The second one was written poorly and the push operation is done in $$O(n!)$$. From a behavioral perspective, both are equivalent. They both correctly implement the expected behavior of a Stack, but in a real software program, it would be better to use the first implementation. That’s the reason why some authors include the complexity of the operations as well in the definition of abstract data types.\nSimple examples\nThe Stack is an example of an abstract data type. We can define the behavior of a stack by giving it two operations.\nPush: Add an element on the stack\nPop: Remove the most recently added element and return it\nWith those two operations, we can completely define the Stack. We note that those two operations are essential to a stack to effectively be a stack. We can derive other operations from those two. We can, for example, define the Peek operator from a pop followed by a push and then returning the value that was popped. Peek isn’t essential because it can be derived from other operations.\nDifferences with object-oriented programming\nTo help demonstrate the differences and trade-offs between object-oriented programming and abstract data type. Here is an adapted version of the Shape example from chapter 6 of Clean Code of Robert C. Martin4\nProcedural Shape (modified)\nFirstly, we notice that outside of the class Geometry; Square, Rectangle and Circle are completely opaque. This hides the implementation details of the shapes.\nWe also notice that if we want to add a new shape, we have to modify all the operations of the Geometry class. Here, we would need to add another clause to handle the new kind of shape.\nOn the contrary, if we wanted to add an operation, we would only need to add a method. The types of shape don’t need to change. The process of adding operations is additive, meaning we do not have to modify existing code.\nPolymorphic shapes\nHere, we notice that if we want to add a new type of shape, we don’t have to modify anything, the process is purely additive. We create a new class that implements Shape and that’s it.\nOn the other side, if we want to add an operation to Shape, we would need to modify all existing classes that implement Shape.\nADT and OOP\nBriefly, we can say that it is easy to add operations on ADT without modifying the existing structure. On the other hand, object-oriented code makes it easy to add new representation without modifying existing operations. 4\nAnother way to say it: ADTs make it difficult to add new structures because we would need to modify all existing operations. OO code makes it difficult to add new operations because we would need to modify all existing classes. 4\nOO and ADT are complementary concepts. Sometimes we want to be able to easily add new representation while other times we want to be able to add operation at our heart’s content. Not everything is an object.\nAdvantages of ADT\nADT allows abstraction. As stated above, abstraction allows the programmer to think at a higher level, using tools to solve his problems without being embarrassed by the implementation details. This is achieved by hiding implementation details behind an opaque type. This allows separating the level of abstractions. High-level operations are not punctuated of low-level operations that hinder the understanding of the code.\nMoreover, ADTs allow the implementation to change without changing the calling code because the interface is kept secret. This allows the user code to be protected against changes in the implementation because they are hidden behind a fixed interface.\nAnother implementation of the ADT can be used interchangeably because of their characterizing behaviors. This allows for performance increase when some implementations are faster in some situations without having to rewrite the client code.\nAs stated above, ADTs make it easy to add new operations on a type. In a context where the number of representations won’t change, but you will often add operations, ADTs are preferable to object.\nSummary\nIn this article, we discussed the concept of an abstract data type. They are an important historical step in the way we build software programs. They are complementary to object in the way they make it easy to add operation while objects make it easy to add new representation.\nReferences\nLiskov, Barbara &amp; Zilles, Stephen. Programming with Abstract Data Types. 1974. ↩ ↩2 ↩3\nEdsger W. Dijkstra. Go To Statement Considered Harmful. 1968. ↩\nBarbara Liskov. Keynote: The Power of Abstraction. 2013. ↩ ↩2\nRobert C. Martin. Clean Code. 2008. ↩ ↩2 ↩3 ↩4"},{"url":"https://willguimont.com/blog/hello-world/","title":"Hello World!","body":"This is the first post on this site."},{"url":"https://willguimont.com/","title":"Home","body":"wigum::intro\nWelcome — I’m William Guimont-Martin (wigum). I am a Ph.D. student in robotics and artificial intelligence, working at the intersection of computer vision and robotics. My research focuses on how we can use additional modalities — such as hyperspectral, polar, or LiDAR data — as scaffolding during training to help models learn more robust and generalizable representations.\nThis website serves as a space to share my work, ideas, and projects — whether they are scientific, technical, or creative. You will find research notes, blog posts, and resources that reflect both my academic path and broader interests in computer science, machine learning, and beyond.\nCheck out the latest posts in the blog, or connect with me on the wider internet through the links in the navigation bar."},{"url":"https://willguimont.com/demo/","title":"Demo Page","body":"Markdown\nText can be bold, italic, strikethrough, and all at the same time.\nLink to another page.\nThere should be whitespace between paragraphs1.\nHeading 1\nHeading 2\nHeading 3\nHeading 4\nHeading 5\nHeading 6\nThis is a normal paragraph2 following a header.\n😭😂🥺🤣❤️✨🙏😍🥰😊\n“Original content is original only for a few seconds before getting old”\nRule #21 of the internet\nItem 1\nItem 2\nItem 2.1\nItem 2.2\nItem 3\nItem 4\nPerform step #1\nProceed to step #2\nConclude with step #3\nMilk\nEggs\nFlour\nCoffee\nCombustible lemons\nMareRatingAdditional info\nFluttershyBest poneShy and adorable\nApple JackGood poneHonest and nice\nPinkie PieFun poneParties and ADHD\nTwilightMain poneNeeerd\nRainbow DashYesLooks badass\nRarityFancy poneGenerous\nDerpy HoovesM u f f i n s[REDACTED]\nExtra\nKaTeX\nDuckquill can render LaTeX using the KaTeX library. It can be enabled using the extra.katex config variable.\n$$\\relax f(x) = \\int_{-\\infty}^\\infty\\hat{f}(\\xi),e^{2 \\pi i \\xi x},d\\xi$$\n$\\relax f(x) = \\int_{-\\infty}^\\infty\\hat{f}(\\xi),e^{2 \\pi i \\xi x},d\\xi$\nShortcodes\nDuckquill provides a few useful shortcodes that simplify some tasks. They can be used on all pages.\nAlerts\nGitHub-style alerts. Simply wrap the text of desired alert inside the shortcode to get the desired look.\nAvailable alert types:\nnote: Useful information that users should know, even when skimming content.\ntip: Helpful advice for doing things better or more easily.\nimportant: Key information users need to know to achieve their goal.\nwarning: Urgent info that needs immediate user attention to avoid problems.\ncaution: Advises about risks or negative outcomes of certain actions.\nUseful information that users should know, even when skimming content.\nHelpful advice for doing things better or more easily.\nKey information users need to know to achieve their goal.\nUrgent info that needs immediate user attention to avoid problems.\nAdvises about risks or negative outcomes of certain actions.\nImages and Videos\nBy default images and videos come with some generic styling, such as rounded corners and shadow. To fine-tune these, you can use shortcodes with different variable combinations.\nAvailable variables are:\nurl: URL to an image.\nurl_min: URL to compressed version of an image, original can be opened by clicking on the image.\nalt: Alt text, same as if the text were inside square brackets in Markdown.\nfull: Forces image to be full-width.\nfull_bleed: Forces image to fill all the available screen width. Removes shadow, rounded corners and zoom on hover.\nstart: Float image to the start of paragraph and scale it down.\nend: Float image to the end of paragraph and scale it down.\npixels: Uses nearest neighbor algorithm for scaling, useful for keeping pixel-art sharp.\ntransparent: Removes rounded corners and shadow, useful for images with transparency.\nno_hover: Removes zoom on hover.\nspoiler: Blurs image until hovered over/pressed on, useful for plot rich game screenshots.\nspoiler with solid: Ditto, but makes the image completely hidden.\nImage with an alt text and without zoom on hover\nImage with compressed version, an alt text, and without zoom on hover\nImage with an alt text, hidden behind a spoiler\nAlternatively, you can append the following URL anchors. It can be more handy in some cases, e.g. such images will render normally in any Markdown editor, opposed to the Zola shortcodes.\n#full: Forces image to be full-width.\n#full-bleed: Forces image to fill all the available screen width. Removes shadow, rounded corners and zoom on hover.\n#start: Float image to the start of paragraph and scale it down.\n#end: Float image to the end of paragraph and scale it down.\n#pixels: Uses nearest neighbor algorithm for scaling, useful for keeping pixel-art sharp.\n#transparent: Removes rounded corners and shadow, useful for images with transparency.\n#no-hover: Removes zoom on hover.\n#spoiler: Blurs image until hovered over/pressed on, useful for plot rich game screenshots.\n#spoiler with #solid: Ditto, but makes the image completely hidden.\nFull-width image with an alt text, pixel-art rendering, no shadow and rounded corners, and no zoom on hover\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magnam aliquam quaerat voluptatem. Ut enim aeque doleamus animo, cum corpore dolemus, fieri tamen permagna accessio potest, si aliquod aeternum et infinitum impendere malum nobis opinemur.\nFor videos it’s all the same except for a few differences: no_hover and url_min variables are not available.\nAdditionally, the following attributes can be set:\nautoplay: Start playing the video automatically.\ncontrols: Display video controls such as volume control, seeking and pause/resume.\nloop: Play the video again once it ends.\nmuted: Turn off the audio by default.\nplaysinline: Prevent the video from playing in fullscreen by default (depends on the browser).\nWebM video example from MDN\nDuckling preening\nCRT\nAlright, this one doesn’t simplify anything, it just adds a CRT-like effect around Markdown code blocks.\nThere’s also a cursor class that you can add to a span with e.g. █ character to simulate the terminal cursor. It doesn’t work from inside Markdown code blocks though.\nYouTube\nAllows to embed a YouTube video using youtube-nocookie.\nAvailable variables are:\nautoplay: Whether the video should autoplay.\nstart: On which second video should start.\nVimeo\nAllows to embed a Vimeo video.\nAvailable variables are:\nautoplay: Whether the video should autoplay.\nMastodon\nAllows to embed a Mastodon post.\nAvailable variables are:\nhost: The instance on which the post resides. If not set, it will fallback to the one set in the [extra.comments] section of config.toml.\nuser: The poster. If not set, it will fallback to the one set in the [extra.comments] section of config.toml.\nid: The ID of the post, usually at the end of the URL.\nDescription List (&lt;dl&gt;)\nName\nGodzilla\nBorn\n1952\nBirthplace\nJapan\nColor\nGreen\nForm Input (&lt;input&gt;)\n&nbsp;Milk\n&nbsp;Eggs\n&nbsp;Flour\n&nbsp;Coffee\n&nbsp;Combustible lemons\nWith switch class:\n&nbsp;Milk\n&nbsp;Eggs\n&nbsp;Flour\n&nbsp;Coffee\n&nbsp;Combustible lemons\nWith switch and big classes:\n&nbsp;Milk\n&nbsp;Eggs\n&nbsp;Flour\n&nbsp;Coffee\n&nbsp;Combustible lemons\nWith radio type:\n&nbsp;Milk\n&nbsp;Eggs\n&nbsp;Flour\n&nbsp;Coffee\n&nbsp;Combustible lemons\nWith color type:\nColor:\nDisabled:\nWith range type:\nFigure Captions (&lt;figcaption&gt;)\nThe Office where Stanley works, it has yellow floor and beige walls\nAccordion (&lt;details&gt;)\nReveal accordion\nGet it? I know, it’s an awful pun.\nSide Comment (&lt;small&gt;)\nSmall, cute text that doesn’t catch attention.\nAbbreviation (&lt;abbr&gt;)\nThe ASCII art is awesome!\nAside (&lt;aside&gt;)\nQuill and a parchment\nA quill is a writing tool made from a moulted flight feather (preferably a primary wing-feather) of a large bird. Quills were used for writing with ink before the invention of the dip pen, the metal-nibbed pen, the fountain pen, and, eventually, the ballpoint pen.\nAs with the earlier reed pen (and later dip pen), a quill has no internal ink reservoir and therefore needs to periodically be dipped into an inkwell during writing. The hand-cut goose quill is rarely used as a calligraphy tool anymore because many papers are now derived from wood pulp and would quickly wear a quill down. However it is still the tool of choice for a few scribes who have noted that quills provide an unmatched sharp stroke as well as greater flexibility than a steel pen.\nKeyboard Input (&lt;kbd&gt;)\nTo switch the keyboard layout, press ⌘ Super + Space.\nMark Text (&lt;mark&gt;)\nYou know what? I’m gonna say some very important stuff, so important that even bold is not enough.\nDeleted and Inserted Text (&lt;del&gt; and &lt;ins&gt;)\nText deleted Text added\nProgress Indicator (&lt;progress&gt;)\nSample Output (&lt;samp&gt;)\nSample Output\nInline Quotation (&lt;q&gt;)\nBlah blah Inline Quote hmm.\nUnarticulated Annotation (&lt;u&gt;)\nYeet the sus drip while vibing with the TikTok fam on a cap-free boomerang.\nExternal Links\nLink to site\nSpoilers\nYou know, Duckquill is a pretty dumb name. I know, crazy.\nWith solid class:\nYou know, Duckquill is a pretty dumb name. I know, crazy.\nButtons Dialog\nGo to Top\nExample\nWith centered and big classes:\nDo Something…\nAccent color:\nLight theme\nDark theme\nFootnote ↩\nFootnote (link) ↩"}]